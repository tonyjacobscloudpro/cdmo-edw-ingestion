{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fe84ea-a81f-4100-b68d-fa8a98fbf0f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pip install packages"
    }
   },
   "outputs": [],
   "source": [
    "pip install pyspark faker azure-storage-file-datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d0ceb8-a044-4c8f-9416-a648733cdaa0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import modules, libraries"
    }
   },
   "outputs": [],
   "source": [
    "# Import modules and libraries\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import col, lit, expr, rand, when\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f15f69e-4128-42a9-be5b-4a0b84aecd22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create cdmo daily datasets as csv"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ComprehensiveDataSets\").getOrCreate()\n",
    "\n",
    "# Define constants for categories, formulation types, and ingredients\n",
    "categories = ['Foundation', 'Lipstick', 'Mascara', 'Eyeshadow', 'Blush']\n",
    "formulation_types = ['Liquid', 'Powder', 'Cream', 'Gel', 'Stick']\n",
    "primary_ingredients = ['Shea Butter', 'Hyaluronic Acid', 'Vitamin E', 'Collagen', 'Aloe Vera']\n",
    "status_options = ['Completed', 'In Progress', 'Failed', 'Pending']\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Product Formulations Dataset (20 rows)\n",
    "# ---------------------------------------------\n",
    "\n",
    "product_formulations_df = (\n",
    "    spark.range(20)\n",
    "    .withColumn(\"ProductID\", expr(\"uuid()\"))\n",
    "    .withColumn(\"ProductName\", expr(\"concat(upper(substring(rand(), 3, 1)), ' ', element_at(array('Foundation', 'Lipstick', 'Mascara', 'Eyeshadow', 'Blush'), (cast(rand()*5+1 as int))) )\"))\n",
    "    .withColumn(\"Category\", expr(\"element_at(array('Foundation', 'Lipstick', 'Mascara', 'Eyeshadow', 'Blush'), (cast(rand()*5+1 as int)))\"))\n",
    "    .withColumn(\"FormulationType\", expr(\"element_at(array('Liquid', 'Powder', 'Cream', 'Gel', 'Stick'), (cast(rand()*5+1 as int)))\"))\n",
    "    .withColumn(\"PrimaryIngredients\", expr(\"concat(element_at(array('Shea Butter', 'Hyaluronic Acid', 'Vitamin E', 'Collagen', 'Aloe Vera'), (cast(rand()*5+1 as int))), ', ', element_at(array('Shea Butter', 'Hyaluronic Acid', 'Vitamin E', 'Collagen', 'Aloe Vera'), (cast(rand()*5+1 as int))))\"))\n",
    "    .withColumn(\"LaunchDate\", expr(\"date_sub(current_date(), cast(rand()*730 as int))\"))  # Random date within the last 2 years\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Manufacturing Batch Dataset (50 rows)\n",
    "# ---------------------------------------------\n",
    "\n",
    "manufacturing_batch_df = (\n",
    "    spark.range(50)\n",
    "    .withColumn(\"BatchID\", expr(\"uuid()\"))\n",
    "    .withColumn(\"ProductID\", expr(\"uuid()\"))  # Link to ProductID\n",
    "    .withColumn(\"BatchDate\", expr(\"date_sub(current_date(), cast(rand()*365 as int))\"))  # Within 1 year\n",
    "    .withColumn(\"Quantity\", (rand() * 900 + 100).cast(\"int\"))\n",
    "    .withColumn(\"Status\", expr(\"element_at(array('Completed', 'In Progress', 'Failed', 'Pending'), (cast(rand()*4+1 as int)))\"))\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Customer Feedback Dataset (50 rows)\n",
    "# ---------------------------------------------\n",
    "\n",
    "customer_feedback_df = (\n",
    "    spark.range(50)\n",
    "    .withColumn(\"FeedbackID\", expr(\"uuid()\"))\n",
    "    .withColumn(\"ProductID\", expr(\"uuid()\"))\n",
    "    .withColumn(\"CustomerID\", expr(\"uuid()\"))\n",
    "    .withColumn(\"Rating\", (rand() * 4 + 1).cast(\"int\"))\n",
    "    .withColumn(\"Comments\", lit(\"Feedback \" + expr(\"cast(rand()*10000 as int)\")))\n",
    "    .withColumn(\"FeedbackDate\", expr(\"date_sub(current_date(), cast(rand()*365 as int))\"))\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Sales Dataset (50 rows)\n",
    "# ---------------------------------------------\n",
    "\n",
    "sales_df = (\n",
    "    spark.range(50)\n",
    "    .withColumn(\"OrderID\", expr(\"uuid()\"))\n",
    "    .withColumn(\"CustomerID\", expr(\"uuid()\"))\n",
    "    .withColumn(\"ProductID\", expr(\"uuid()\"))\n",
    "    .withColumn(\"Quantity\", (rand() * 9 + 1).cast(\"int\"))\n",
    "    .withColumn(\"TotalAmount\", (rand() * 480 + 20).cast(\"float\"))\n",
    "    .withColumn(\"OrderDate\", expr(\"date_sub(current_date(), cast(rand()*365 as int))\"))\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Supplier Information Dataset (20 rows)\n",
    "# ---------------------------------------------\n",
    "\n",
    "supplier_df = (\n",
    "    spark.range(20)\n",
    "    .withColumn(\"SupplierID\", expr(\"uuid()\"))\n",
    "    .withColumn(\"SupplierName\", lit(\"Supplier_\" + expr(\"cast(rand()*1000 as int)\")))\n",
    "    .withColumn(\"Material\", expr(\"element_at(array('Shea Butter', 'Hyaluronic Acid', 'Vitamin E', 'Collagen', 'Aloe Vera'), (cast(rand()*5+1 as int)))\"))\n",
    "    .withColumn(\"Cost\", (rand() * 90 + 10).cast(\"float\"))\n",
    "    .withColumn(\"DeliveryDate\", expr(\"date_sub(current_date(), cast(rand()*365 as int))\"))\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Save DataFrames to CSV Files in Databricks\n",
    "# ---------------------------------------------\n",
    "\n",
    "output_path = \"dbfs:/tmp/\"\n",
    "\n",
    "product_formulations_df.coalesce(1).write.csv(output_path + \"product_formulations.csv\", header=True, mode=\"overwrite\")\n",
    "manufacturing_batch_df.coalesce(1).write.csv(output_path + \"manufacturing_batches.csv\", header=True, mode=\"overwrite\")\n",
    "customer_feedback_df.coalesce(1).write.csv(output_path + \"customer_feedback.csv\", header=True, mode=\"overwrite\")\n",
    "sales_df.coalesce(1).write.csv(output_path + \"sales_data.csv\", header=True, mode=\"overwrite\")\n",
    "supplier_df.coalesce(1).write.csv(output_path + \"supplier_information.csv\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41e487f-812b-40aa-b0c4-c4b6d9ac9d72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Upload csv datasets to landing zone"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded file to ADLS: data/incoming/customerfeedback/customerfeedback_20250130_044103.csv\n✅ Uploaded file to ADLS: data/archive/customerfeedback/customerfeedback_20250130_044103.csv\n✅ Dataset 'customerfeedback' successfully uploaded to ADLS (incoming & archive).\n✅ Uploaded file to ADLS: data/incoming/manufacturebatch/manufacturebatch_20250130_044103.csv\n✅ Uploaded file to ADLS: data/archive/manufacturebatch/manufacturebatch_20250130_044103.csv\n✅ Dataset 'manufacturebatch' successfully uploaded to ADLS (incoming & archive).\n✅ Uploaded file to ADLS: data/incoming/productformula/productformula_20250130_044103.csv\n✅ Uploaded file to ADLS: data/archive/productformula/productformula_20250130_044103.csv\n✅ Dataset 'productformula' successfully uploaded to ADLS (incoming & archive).\n✅ Uploaded file to ADLS: data/incoming/sales/sales_20250130_044103.csv\n✅ Uploaded file to ADLS: data/archive/sales/sales_20250130_044103.csv\n✅ Dataset 'sales' successfully uploaded to ADLS (incoming & archive).\n✅ Uploaded file to ADLS: data/incoming/supplier/supplier_20250130_044103.csv\n✅ Uploaded file to ADLS: data/archive/supplier/supplier_20250130_044103.csv\n✅ Dataset 'supplier' successfully uploaded to ADLS (incoming & archive).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# 1. AZURE STORAGE CONFIGURATION (USING SAS TOKEN)\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Azure Storage connection details\n",
    "account_name = \"cdmo\"\n",
    "sas_token = \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2025-01-30T10:43:36Z&st=2025-01-30T02:43:36Z&spr=https&sig=nYI7xcNkNktzxh0x0JaQKh0FYt9oPMMrKv%2Bkv6EFGTI%3D\"  # Replace with your SAS token\n",
    "container_name = \"00-landing\"\n",
    "\n",
    "# Construct the BlobServiceClient URL\n",
    "blob_service_url = f\"https://{account_name}.blob.core.windows.net?{sas_token}\"\n",
    "\n",
    "# Initialize BlobServiceClient\n",
    "blob_service_client = BlobServiceClient(account_url=blob_service_url)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# 2. FUNCTION TO UPLOAD A FILE TO ADLS\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "def upload_to_adls(local_path, blob_path):\n",
    "    \"\"\"\n",
    "    Uploads a file to Azure Data Lake Storage using SAS Token authentication.\n",
    "\n",
    "    Args:\n",
    "        local_path (str): Local file path to upload.\n",
    "        blob_path (str): Path in the ADLS container to upload the file to.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the BlobClient for the target file in the specified container\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)\n",
    "\n",
    "        # Upload the file to ADLS\n",
    "        with open(local_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "        print(f\"✅ Uploaded file to ADLS: {blob_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error uploading file to ADLS: {e}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# 3. DATASETS & FILE PROCESSING\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Define datasets and their corresponding ADLS folders\n",
    "datasets = {\n",
    "    \"customerfeedback\": customer_feedback_df,\n",
    "    \"manufacturebatch\": manufacturing_batch_df,\n",
    "    \"productformula\": product_formulations_df,\n",
    "    \"sales\": sales_df,\n",
    "    \"supplier\": supplier_df,\n",
    "}\n",
    "\n",
    "# Get the current date and time for file versioning\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "current_time = datetime.now().strftime(\"%H%M%S\")\n",
    "\n",
    "# Process each dataset and upload it to both incoming and archive paths\n",
    "for dataset_name, dataframe in datasets.items():\n",
    "    try:\n",
    "        # Define temporary paths for saving and accessing data\n",
    "        dbfs_temp_dir = f\"dbfs:/tmp/{dataset_name}\"  # Temporary DBFS directory for Spark output\n",
    "        local_temp_dir = f\"/dbfs/tmp/{dataset_name}\"  # Local path to access DBFS files\n",
    "        os.makedirs(local_temp_dir, exist_ok=True)\n",
    "\n",
    "        # Save the DataFrame to DBFS as a single CSV file\n",
    "        dataframe.coalesce(1).write.csv(dbfs_temp_dir, header=True, mode=\"overwrite\")\n",
    "\n",
    "        # Locate the part file in the DBFS directory\n",
    "        files = dbutils.fs.ls(dbfs_temp_dir)\n",
    "        part_file = next(f.path for f in files if f.name.startswith(\"part-\"))\n",
    "\n",
    "        # Rename the part file to a meaningful name with date and time\n",
    "        local_file_path = os.path.join(local_temp_dir, f\"{dataset_name}_{current_date}_{current_time}.csv\")\n",
    "        dbutils.fs.cp(part_file, f\"file:{local_file_path}\")\n",
    "\n",
    "        # Define the blob paths in ADLS for both incoming and archive\n",
    "        incoming_blob_path = f\"data/incoming/{dataset_name}/{dataset_name}_{current_date}_{current_time}.csv\"\n",
    "        archive_blob_path = f\"data/archive/{dataset_name}/{dataset_name}_{current_date}_{current_time}.csv\"\n",
    "\n",
    "        # Upload the file to both incoming and archive directories\n",
    "        upload_to_adls(local_file_path, incoming_blob_path)\n",
    "        upload_to_adls(local_file_path, archive_blob_path)\n",
    "\n",
    "        # Clean up the temporary DBFS directory\n",
    "        dbutils.fs.rm(dbfs_temp_dir, recurse=True)\n",
    "\n",
    "        print(f\"✅ Dataset '{dataset_name}' successfully uploaded to ADLS (incoming & archive).\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing dataset '{dataset_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695ce2a3-946f-40cd-8f58-8b408fd635d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validate Datasets"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files in container: 00-landing\n- data/archive/customerfeedback/customerfeedback_20250130_044103.csv\n- data/archive/manufacturebatch/manufacturebatch_20250130_044103.csv\n- data/archive/productformula/productformula_20250130_044103.csv\n- data/archive/sales/sales_20250130_044103.csv\n- data/archive/supplier/supplier_20250130_044103.csv\n- data/incoming/customerfeedback/customerfeedback_20250130_044103.csv\n- data/incoming/manufacturebatch/manufacturebatch_20250130_044103.csv\n- data/incoming/productformula/productformula_20250130_044103.csv\n- data/incoming/sales/sales_20250130_044103.csv\n- data/incoming/supplier/supplier_20250130_044103.csv\n"
     ]
    }
   ],
   "source": [
    "# Validate files in landing zone were created\n",
    "def get_adls_service_client(account_name, sas_token):\n",
    "    \"\"\"Authenticate using SAS token and return an ADLS service client.\"\"\"\n",
    "    account_url = f\"https://{account_name}.dfs.core.windows.net\"\n",
    "    return DataLakeServiceClient(account_url, credential=sas_token)\n",
    "\n",
    "def list_files_in_container(service_client, container_name):\n",
    "    \"\"\"List all files in the specified ADLS container.\"\"\"\n",
    "    file_system_client = service_client.get_file_system_client(file_system=container_name)\n",
    "    \n",
    "    print(f\"Listing files in container: {container_name}\")\n",
    "    paths = file_system_client.get_paths()\n",
    "\n",
    "    file_list = [path.name for path in paths if not path.is_directory]\n",
    "    \n",
    "    if file_list:\n",
    "        for file in file_list:\n",
    "            print(f\"- {file}\")\n",
    "    else:\n",
    "        print(\"No files found.\")\n",
    "\n",
    "    return file_list\n",
    "\n",
    "# Connect and list files\n",
    "try:\n",
    "    adls_client = get_adls_service_client(account_name, sas_token)\n",
    "    list_files_in_container(adls_client, \"00-landing\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-cdmo-datasets-creation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
