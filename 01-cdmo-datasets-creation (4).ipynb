{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fe84ea-a81f-4100-b68d-fa8a98fbf0f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pip install packages"
    }
   },
   "outputs": [],
   "source": [
    "pip install pyspark faker azure-storage-file-datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d0ceb8-a044-4c8f-9416-a648733cdaa0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import modules, libraries"
    }
   },
   "outputs": [],
   "source": [
    "# Import modules and libraries\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f15f69e-4128-42a9-be5b-4a0b84aecd22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create cdmo daily datasets as csv"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ComprehensiveDataSets\").getOrCreate()\n",
    "\n",
    "# Initialize Faker for generating mock data\n",
    "fake = Faker()\n",
    "\n",
    "# Define categories and formulation types for the dataset\n",
    "categories = ['Foundation', 'Lipstick', 'Mascara', 'Eyeshadow', 'Blush']\n",
    "formulation_types = ['Liquid', 'Powder', 'Cream', 'Gel', 'Stick']\n",
    "primary_ingredients = ['Shea Butter', 'Hyaluronic Acid', 'Vitamin E', 'Collagen', 'Aloe Vera']\n",
    "status_options = ['Completed', 'In Progress', 'Failed', 'Pending']\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Product Formulations Dataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a list to store product data\n",
    "product_data = []\n",
    "product_ids = []  # To store unique ProductIDs for relationships\n",
    "\n",
    "# Generate 20 unique product formulations\n",
    "for _ in range(20):\n",
    "    product_id = fake.uuid4()  # Unique identifier for each product\n",
    "    product_ids.append(product_id)\n",
    "    product = {\n",
    "        \"ProductID\": product_id,\n",
    "        \"ProductName\": fake.word().capitalize() + \" \" + random.choice(categories),\n",
    "        \"Category\": random.choice(categories),\n",
    "        \"FormulationType\": random.choice(formulation_types),\n",
    "        \"PrimaryIngredients\": ', '.join(random.sample(primary_ingredients, 2)),\n",
    "        \"LaunchDate\": fake.date_between(start_date=\"-2y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    product_data.append(product)\n",
    "\n",
    "# Define schema for Product Formulations\n",
    "product_schema = StructType([\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"FormulationType\", StringType(), True),\n",
    "    StructField(\"PrimaryIngredients\", StringType(), True),\n",
    "    StructField(\"LaunchDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Product Formulations DataFrame\n",
    "product_formulations_df = spark.createDataFrame(product_data, product_schema)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Manufacturing Batch Dataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a list to store batch data\n",
    "batch_data = []\n",
    "\n",
    "# Generate 50 manufacturing batches\n",
    "for _ in range(50):\n",
    "    batch = {\n",
    "        \"BatchID\": fake.uuid4(),\n",
    "        \"ProductID\": random.choice(product_ids),  # Associate with a product\n",
    "        \"BatchDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\"),\n",
    "        \"Quantity\": random.randint(100, 1000),\n",
    "        \"Status\": random.choice(status_options)\n",
    "    }\n",
    "    batch_data.append(batch)\n",
    "\n",
    "# Define schema for Manufacturing Batches\n",
    "batch_schema = StructType([\n",
    "    StructField(\"BatchID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"BatchDate\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Manufacturing Batches DataFrame\n",
    "manufacturing_batch_df = spark.createDataFrame(batch_data, batch_schema)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Customer Feedback Dataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a list to store customer feedback data\n",
    "customer_feedback_data = []\n",
    "customer_ids = [fake.uuid4() for _ in range(30)]  # Generate 30 unique Customer IDs\n",
    "\n",
    "# Generate 50 customer feedback entries\n",
    "for _ in range(50):\n",
    "    feedback = {\n",
    "        \"FeedbackID\": fake.uuid4(),\n",
    "        \"ProductID\": random.choice(product_ids),\n",
    "        \"CustomerID\": random.choice(customer_ids),\n",
    "        \"Rating\": random.randint(1, 5),\n",
    "        \"Comments\": fake.sentence(),\n",
    "        \"FeedbackDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    customer_feedback_data.append(feedback)\n",
    "\n",
    "# Define schema for Customer Feedback\n",
    "customer_feedback_schema = StructType([\n",
    "    StructField(\"FeedbackID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"Rating\", IntegerType(), True),\n",
    "    StructField(\"Comments\", StringType(), True),\n",
    "    StructField(\"FeedbackDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Customer Feedback DataFrame\n",
    "customer_feedback_df = spark.createDataFrame(customer_feedback_data, customer_feedback_schema)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Sales Dataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a list to store sales data\n",
    "sales_data = []\n",
    "\n",
    "# Generate 50 sales transactions\n",
    "for _ in range(50):\n",
    "    sale = {\n",
    "        \"OrderID\": fake.uuid4(),\n",
    "        \"CustomerID\": random.choice(customer_ids),\n",
    "        \"ProductID\": random.choice(product_ids),\n",
    "        \"Quantity\": random.randint(1, 10),\n",
    "        \"TotalAmount\": round(random.uniform(20, 500), 2),\n",
    "        \"OrderDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    sales_data.append(sale)\n",
    "\n",
    "# Define schema for Sales Data\n",
    "sales_schema = StructType([\n",
    "    StructField(\"OrderID\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"TotalAmount\", FloatType(), True),\n",
    "    StructField(\"OrderDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Sales Data DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Supplier Information Dataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a list to store supplier data\n",
    "supplier_data = []\n",
    "supplier_ids = [fake.uuid4() for _ in range(10)]  # Generate 10 unique Supplier IDs\n",
    "\n",
    "# Generate 20 supplier records\n",
    "for _ in range(20):\n",
    "    supplier = {\n",
    "        \"SupplierID\": random.choice(supplier_ids),\n",
    "        \"SupplierName\": fake.company(),\n",
    "        \"Material\": random.choice(primary_ingredients),\n",
    "        \"Cost\": round(random.uniform(10, 100), 2),\n",
    "        \"DeliveryDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    supplier_data.append(supplier)\n",
    "\n",
    "# Define schema for Supplier Information\n",
    "supplier_schema = StructType([\n",
    "    StructField(\"SupplierID\", StringType(), True),\n",
    "    StructField(\"SupplierName\", StringType(), True),\n",
    "    StructField(\"Material\", StringType(), True),\n",
    "    StructField(\"Cost\", FloatType(), True),\n",
    "    StructField(\"DeliveryDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Supplier Information DataFrame\n",
    "supplier_df = spark.createDataFrame(supplier_data, supplier_schema)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Save DataFrames to CSV Files\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Define base path for saving files\n",
    "output_path = \"dbfs:/tmp/\"\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "product_formulations_df.coalesce(1).write.csv(output_path + \"product_formulations.csv\", header=True, mode=\"overwrite\")\n",
    "manufacturing_batch_df.coalesce(1).write.csv(output_path + \"manufacturing_batches.csv\", header=True, mode=\"overwrite\")\n",
    "customer_feedback_df.coalesce(1).write.csv(output_path + \"customer_feedback.csv\", header=True, mode=\"overwrite\")\n",
    "sales_df.coalesce(1).write.csv(output_path + \"sales_data.csv\", header=True, mode=\"overwrite\")\n",
    "supplier_df.coalesce(1).write.csv(output_path + \"supplier_information.csv\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41e487f-812b-40aa-b0c4-c4b6d9ac9d72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Upload csv datasets to landing zone"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded file to ADLS: data/incoming/customerfeedback/customerfeedback_20250129_205255.csv\n✅ Dataset 'customerfeedback' successfully uploaded to ADLS as data/incoming/customerfeedback/customerfeedback_20250129_205255.csv.\n✅ Uploaded file to ADLS: data/incoming/manufacturebatch/manufacturebatch_20250129_205255.csv\n✅ Dataset 'manufacturebatch' successfully uploaded to ADLS as data/incoming/manufacturebatch/manufacturebatch_20250129_205255.csv.\n✅ Uploaded file to ADLS: data/incoming/productformula/productformula_20250129_205255.csv\n✅ Dataset 'productformula' successfully uploaded to ADLS as data/incoming/productformula/productformula_20250129_205255.csv.\n✅ Uploaded file to ADLS: data/incoming/sales/sales_20250129_205255.csv\n✅ Dataset 'sales' successfully uploaded to ADLS as data/incoming/sales/sales_20250129_205255.csv.\n✅ Uploaded file to ADLS: data/incoming/supplier/supplier_20250129_205255.csv\n✅ Dataset 'supplier' successfully uploaded to ADLS as data/incoming/supplier/supplier_20250129_205255.csv.\n"
     ]
    }
   ],
   "source": [
    "# Azure Storage connection details (using SAS Token for secure access)\n",
    "account_name = \"cdmo\"\n",
    "sas_token = \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2025-01-30T04:19:28Z&st=2025-01-29T20:19:28Z&spr=https&sig=ZhvYNtj9coHvI6vw0JWdayJ29mNbpoOq9d%2B4oR2zEwQ%3D\"   # Replace with your SAS token\n",
    "container_name = \"00-landing\"\n",
    "\n",
    "# Construct the BlobServiceClient URL\n",
    "blob_service_url = f\"https://{account_name}.blob.core.windows.net?{sas_token}\"\n",
    "\n",
    "# Function to upload a file to Azure Data Lake Storage (ADLS)\n",
    "def upload_to_adls(local_path, blob_path):\n",
    "    \"\"\"\n",
    "    Uploads a file to Azure Data Lake Storage using SAS Token authentication.\n",
    "\n",
    "    Args:\n",
    "        local_path (str): Local file path to upload.\n",
    "        blob_path (str): Path in the ADLS container to upload the file to.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize BlobServiceClient using the SAS token URL\n",
    "        blob_service_client = BlobServiceClient(account_url=blob_service_url)\n",
    "\n",
    "        # Get the BlobClient for the target file in the specified container\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)\n",
    "\n",
    "        # Upload the file to ADLS\n",
    "        with open(local_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "        print(f\"✅ Uploaded file to ADLS: {blob_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error uploading file to ADLS: {e}\")\n",
    "\n",
    "# Define datasets and their corresponding ADLS folders\n",
    "datasets = {\n",
    "    \"customerfeedback\": customer_feedback_df,\n",
    "    \"manufacturebatch\": manufacturing_batch_df,\n",
    "    \"productformula\": product_formulations_df,\n",
    "    \"sales\": sales_df,\n",
    "    \"supplier\": supplier_df,\n",
    "}\n",
    "\n",
    "# Get the current date and time for file versioning\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "current_time = datetime.now().strftime(\"%H%M%S\")\n",
    "\n",
    "# Save DataFrames to DBFS and upload to ADLS\n",
    "for dataset_name, dataframe in datasets.items():\n",
    "    try:\n",
    "        # Define temporary paths for saving and accessing data\n",
    "        dbfs_temp_dir = f\"dbfs:/tmp/{dataset_name}\"  # Temporary DBFS directory for Spark output\n",
    "        local_temp_dir = f\"/dbfs/tmp/{dataset_name}\"  # Local path to access DBFS files\n",
    "        os.makedirs(local_temp_dir, exist_ok=True)\n",
    "\n",
    "        # Save the DataFrame to DBFS as a single CSV file\n",
    "        dataframe.coalesce(1).write.csv(dbfs_temp_dir, header=True, mode=\"overwrite\")\n",
    "\n",
    "        # Locate the part file in the DBFS directory\n",
    "        files = dbutils.fs.ls(dbfs_temp_dir)\n",
    "        part_file = next(f.path for f in files if f.name.startswith(\"part-\"))\n",
    "\n",
    "        # Rename the part file to a meaningful name with date and time\n",
    "        local_file_path = os.path.join(local_temp_dir, f\"{dataset_name}_{current_date}_{current_time}.csv\")\n",
    "        dbutils.fs.cp(part_file, f\"file:{local_file_path}\")\n",
    "\n",
    "        # Define the blob path in ADLS with the date folder and timestamped file name\n",
    "        adls_blob_path = f\"data/incoming/{dataset_name}/{dataset_name}_{current_date}_{current_time}.csv\"\n",
    "\n",
    "        # Upload the file to ADLS\n",
    "        upload_to_adls(local_file_path, adls_blob_path)\n",
    "\n",
    "        # Clean up the temporary DBFS directory\n",
    "        dbutils.fs.rm(dbfs_temp_dir, recurse=True)\n",
    "\n",
    "        print(f\"✅ Dataset '{dataset_name}' successfully uploaded to ADLS as {adls_blob_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing dataset '{dataset_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695ce2a3-946f-40cd-8f58-8b408fd635d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validate Datasets"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files in container: 00-landing\n- data/incoming/customerfeedback/customerfeedback_20250129_205255.csv\n- data/incoming/manufacturebatch/manufacturebatch_20250129_205255.csv\n- data/incoming/productformula/productformula_20250129_205255.csv\n- data/incoming/sales/sales_20250129_205255.csv\n- data/incoming/supplier/supplier_20250129_205255.csv\n"
     ]
    }
   ],
   "source": [
    "# Validate files in landing zone were created\n",
    "def get_adls_service_client(account_name, sas_token):\n",
    "    \"\"\"Authenticate using SAS token and return an ADLS service client.\"\"\"\n",
    "    account_url = f\"https://{account_name}.dfs.core.windows.net\"\n",
    "    return DataLakeServiceClient(account_url, credential=sas_token)\n",
    "\n",
    "def list_files_in_container(service_client, container_name):\n",
    "    \"\"\"List all files in the specified ADLS container.\"\"\"\n",
    "    file_system_client = service_client.get_file_system_client(file_system=container_name)\n",
    "    \n",
    "    print(f\"Listing files in container: {container_name}\")\n",
    "    paths = file_system_client.get_paths()\n",
    "\n",
    "    file_list = [path.name for path in paths if not path.is_directory]\n",
    "    \n",
    "    if file_list:\n",
    "        for file in file_list:\n",
    "            print(f\"- {file}\")\n",
    "    else:\n",
    "        print(\"No files found.\")\n",
    "\n",
    "    return file_list\n",
    "\n",
    "# Connect and list files\n",
    "try:\n",
    "    adls_client = get_adls_service_client(account_name, sas_token)\n",
    "    list_files_in_container(adls_client, \"00-landing\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-cdmo-datasets-creation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
