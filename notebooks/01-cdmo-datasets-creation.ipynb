{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fe84ea-a81f-4100-b68d-fa8a98fbf0f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pip install packages"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting pyspark\n  Using cached pyspark-3.5.4-py2.py3-none-any.whl\nCollecting faker\n  Using cached Faker-35.0.0-py3-none-any.whl (1.9 MB)\nCollecting azure-storage-file-datalake\n  Using cached azure_storage_file_datalake-12.18.1-py3-none-any.whl (258 kB)\nRequirement already satisfied: py4j==0.10.9.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2215ca21-9e0e-46e6-aee6-2e5e7f52dde3/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\nRequirement already satisfied: python-dateutil>=2.4 in /databricks/python3/lib/python3.9/site-packages (from faker) (2.8.2)\nRequirement already satisfied: typing-extensions in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2215ca21-9e0e-46e6-aee6-2e5e7f52dde3/lib/python3.9/site-packages (from faker) (4.12.2)\nRequirement already satisfied: azure-storage-blob>=12.24.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2215ca21-9e0e-46e6-aee6-2e5e7f52dde3/lib/python3.9/site-packages (from azure-storage-file-datalake) (12.24.1)\nRequirement already satisfied: azure-core>=1.30.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2215ca21-9e0e-46e6-aee6-2e5e7f52dde3/lib/python3.9/site-packages (from azure-storage-file-datalake) (1.32.0)\nRequirement already satisfied: isodate>=0.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2215ca21-9e0e-46e6-aee6-2e5e7f52dde3/lib/python3.9/site-packages (from azure-storage-file-datalake) (0.7.2)\nRequirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core>=1.30.0->azure-storage-file-datalake) (1.16.0)\nRequirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core>=1.30.0->azure-storage-file-datalake) (2.27.1)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.9/site-packages (from azure-storage-blob>=12.24.1->azure-storage-file-datalake) (3.4.8)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.9/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.24.1->azure-storage-file-datalake) (1.15.0)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.24.1->azure-storage-file-datalake) (2.21)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (1.26.9)\nInstalling collected packages: pyspark, faker, azure-storage-file-datalake\nSuccessfully installed azure-storage-file-datalake-12.18.1 faker-35.0.0 pyspark-3.5.4\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark faker azure-storage-file-datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d0ceb8-a044-4c8f-9416-a648733cdaa0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import modules, libraries, classes, and functions"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f15f69e-4128-42a9-be5b-4a0b84aecd22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create cdmo daily datasets as csv"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ComprehensiveDataSets\").getOrCreate()\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Categories and formulation types\n",
    "categories = ['Foundation', 'Lipstick', 'Mascara', 'Eyeshadow', 'Blush']\n",
    "formulation_types = ['Liquid', 'Powder', 'Cream', 'Gel', 'Stick']\n",
    "primary_ingredients = ['Shea Butter', 'Hyaluronic Acid', 'Vitamin E', 'Collagen', 'Aloe Vera']\n",
    "status_options = ['Completed', 'In Progress', 'Failed', 'Pending']\n",
    "\n",
    "# Generate Product Formulations\n",
    "product_data = []\n",
    "product_ids = []  # To store ProductIDs for relationships\n",
    "for _ in range(20):\n",
    "    product_id = fake.uuid4()\n",
    "    product_ids.append(product_id)\n",
    "    product = {\n",
    "        \"ProductID\": product_id,\n",
    "        \"ProductName\": fake.word().capitalize() + \" \" + random.choice(categories),\n",
    "        \"Category\": random.choice(categories),\n",
    "        \"FormulationType\": random.choice(formulation_types),\n",
    "        \"PrimaryIngredients\": ', '.join(random.sample(primary_ingredients, 2)),\n",
    "        \"LaunchDate\": fake.date_between(start_date=\"-2y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    product_data.append(product)\n",
    "\n",
    "# Define schema for Product Formulations\n",
    "product_schema = StructType([\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"FormulationType\", StringType(), True),\n",
    "    StructField(\"PrimaryIngredients\", StringType(), True),\n",
    "    StructField(\"LaunchDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Product Formulations DataFrame\n",
    "product_formulations_df = spark.createDataFrame(product_data, product_schema)\n",
    "\n",
    "# Generate Manufacturing Batch Data\n",
    "batch_data = []\n",
    "for _ in range(50):\n",
    "    batch = {\n",
    "        \"BatchID\": fake.uuid4(),\n",
    "        \"ProductID\": random.choice(product_ids),\n",
    "        \"BatchDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\"),\n",
    "        \"Quantity\": random.randint(100, 1000),\n",
    "        \"Status\": random.choice(status_options)\n",
    "    }\n",
    "    batch_data.append(batch)\n",
    "\n",
    "# Define schema for Manufacturing Batches\n",
    "batch_schema = StructType([\n",
    "    StructField(\"BatchID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"BatchDate\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Manufacturing Batches DataFrame\n",
    "manufacturing_batch_df = spark.createDataFrame(batch_data, batch_schema)\n",
    "\n",
    "# Generate Customer Feedback Data\n",
    "customer_feedback_data = []\n",
    "customer_ids = [fake.uuid4() for _ in range(30)]  # Generate unique Customer IDs\n",
    "for _ in range(50):\n",
    "    feedback = {\n",
    "        \"FeedbackID\": fake.uuid4(),\n",
    "        \"ProductID\": random.choice(product_ids),\n",
    "        \"CustomerID\": random.choice(customer_ids),\n",
    "        \"Rating\": random.randint(1, 5),\n",
    "        \"Comments\": fake.sentence(),\n",
    "        \"FeedbackDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    customer_feedback_data.append(feedback)\n",
    "\n",
    "# Define schema for Customer Feedback\n",
    "customer_feedback_schema = StructType([\n",
    "    StructField(\"FeedbackID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"Rating\", IntegerType(), True),\n",
    "    StructField(\"Comments\", StringType(), True),\n",
    "    StructField(\"FeedbackDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Customer Feedback DataFrame\n",
    "customer_feedback_df = spark.createDataFrame(customer_feedback_data, customer_feedback_schema)\n",
    "\n",
    "# Generate Sales Data\n",
    "sales_data = []\n",
    "for _ in range(50):\n",
    "    sale = {\n",
    "        \"OrderID\": fake.uuid4(),\n",
    "        \"CustomerID\": random.choice(customer_ids),\n",
    "        \"ProductID\": random.choice(product_ids),\n",
    "        \"Quantity\": random.randint(1, 10),\n",
    "        \"TotalAmount\": round(random.uniform(20, 500), 2),\n",
    "        \"OrderDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    sales_data.append(sale)\n",
    "\n",
    "# Define schema for Sales Data\n",
    "sales_schema = StructType([\n",
    "    StructField(\"OrderID\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"TotalAmount\", FloatType(), True),\n",
    "    StructField(\"OrderDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Sales Data DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# Generate Supplier Information Data\n",
    "supplier_data = []\n",
    "supplier_ids = [fake.uuid4() for _ in range(10)]  # Generate unique Supplier IDs\n",
    "for _ in range(20):\n",
    "    supplier = {\n",
    "        \"SupplierID\": random.choice(supplier_ids),\n",
    "        \"SupplierName\": fake.company(),\n",
    "        \"Material\": random.choice(primary_ingredients),\n",
    "        \"Cost\": round(random.uniform(10, 100), 2),\n",
    "        \"DeliveryDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    supplier_data.append(supplier)\n",
    "\n",
    "# Define schema for Supplier Information\n",
    "supplier_schema = StructType([\n",
    "    StructField(\"SupplierID\", StringType(), True),\n",
    "    StructField(\"SupplierName\", StringType(), True),\n",
    "    StructField(\"Material\", StringType(), True),\n",
    "    StructField(\"Cost\", FloatType(), True),\n",
    "    StructField(\"DeliveryDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Supplier Information DataFrame\n",
    "supplier_df = spark.createDataFrame(supplier_data, supplier_schema)\n",
    "\n",
    "# Display the DataFrames\n",
    "# print(\"Product Formulations Dataset:\")\n",
    "# product_formulations_df.display()\n",
    "\n",
    "# print(\"Manufacturing Batches Dataset:\")\n",
    "# manufacturing_batch_df.display()\n",
    "\n",
    "# print(\"Customer Feedback Dataset:\")\n",
    "# customer_feedback_df.display()\n",
    "\n",
    "# print(\"Sales Dataset:\")\n",
    "# sales_df.display()\n",
    "\n",
    "# print(\"Supplier Information Dataset:\")\n",
    "# supplier_df.display()\n",
    "\n",
    "# Save DataFrames to CSV\n",
    "product_formulations_df.coalesce(1).write.csv(\"dbfs:/tmp/product_formulations.csv\", header=True, mode=\"overwrite\")\n",
    "manufacturing_batch_df.coalesce(1).write.csv(\"dbfs:/tmp/manufacturing_batches.csv\", header=True, mode=\"overwrite\")\n",
    "customer_feedback_df.coalesce(1).write.csv(\"dbfs:/tmp/customer_feedback.csv\", header=True, mode=\"overwrite\")\n",
    "sales_df.coalesce(1).write.csv(\"dbfs:/tmp/sales_data.csv\", header=True, mode=\"overwrite\")\n",
    "supplier_df.coalesce(1).write.csv(\"dbfs:/tmp/supplier_information.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41e487f-812b-40aa-b0c4-c4b6d9ac9d72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Upload csv datasets to landing zone"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Azure Storage connection details\n",
    "connection_string = \"XXXXXXXXXXXXX\"\n",
    "container_name = \"00-landing\"\n",
    "\n",
    "# Function to upload a file to ADLS\n",
    "def upload_to_adls(local_path, blob_path):\n",
    "    try:\n",
    "        # Initialize BlobServiceClient\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "        \n",
    "        # Get the blob client for the target file\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)\n",
    "        \n",
    "        # Upload the file\n",
    "        with open(local_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "        print(f\"Uploaded file to ADLS: {blob_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to ADLS: {e}\")\n",
    "\n",
    "# Define datasets and their corresponding ADLS folders\n",
    "datasets = {\n",
    "    \"customerfeedback\": customer_feedback_df,\n",
    "    \"manufacturebatch\": manufacturing_batch_df,\n",
    "    \"productformula\": product_formulations_df,\n",
    "    \"sales\": sales_df,\n",
    "    \"supplier\": supplier_df,\n",
    "}\n",
    "\n",
    "# Get the current date and time\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "current_time = datetime.now().strftime(\"%H%M%S\")\n",
    "\n",
    "# Save DataFrames to DBFS and upload to ADLS\n",
    "for dataset_name, dataframe in datasets.items():\n",
    "    try:\n",
    "        # Define the DBFS and local paths\n",
    "        dbfs_temp_dir = f\"dbfs:/tmp/{dataset_name}\"  # Temporary directory for Spark output\n",
    "        local_temp_dir = f\"/dbfs/tmp/{dataset_name}\"  # Local path for accessing DBFS files\n",
    "        os.makedirs(local_temp_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the DataFrame to DBFS as a single CSV\n",
    "        dataframe.coalesce(1).write.csv(dbfs_temp_dir, header=True, mode=\"overwrite\")\n",
    "        \n",
    "        # Locate the part file in the DBFS directory\n",
    "        files = dbutils.fs.ls(dbfs_temp_dir)\n",
    "        part_file = next(f.path for f in files if f.name.startswith(\"part-\"))\n",
    "        \n",
    "        # Rename the part file to a meaningful name with date and time\n",
    "        local_file_path = os.path.join(local_temp_dir, f\"{dataset_name}_{current_date}_{current_time}.csv\")\n",
    "        dbutils.fs.cp(part_file, f\"file:{local_file_path}\")\n",
    "        \n",
    "        # Define the blob path in ADLS with the date folder and timestamped file name\n",
    "        adls_incoming_blob_path = f\"data/incoming/{dataset_name}/{dataset_name}_{current_date}_{current_time}.csv\"\n",
    "        adls_archive_blob_path = f\"data/archive/{dataset_name}/{dataset_name}_{current_date}_{current_time}.csv\"\n",
    "        \n",
    "        # Upload the file to ADLS\n",
    "        upload_to_adls(local_file_path, adls_incoming_blob_path)\n",
    "        upload_to_adls(local_file_path, adls_archive_blob_path)\n",
    "        \n",
    "        # Clean up temporary directories\n",
    "        dbutils.fs.rm(dbfs_temp_dir, recurse=True)\n",
    "        print(f\"Dataset '{dataset_name}' successfully uploaded to ADLS as {adls_blob_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset '{dataset_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695ce2a3-946f-40cd-8f58-8b408fd635d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validate Datasets"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found for dataset 'customerfeedback': ['data/incoming/customerfeedback/customerfeedback_20250127_013854.csv']\nValidating dataset: customerfeedback, File: data/incoming/customerfeedback/customerfeedback_20250127_013854.csv\nFiles found for dataset 'manufacturebatch': ['data/incoming/manufacturebatch/manufacturebatch_20250127_013854.csv']\nValidating dataset: manufacturebatch, File: data/incoming/manufacturebatch/manufacturebatch_20250127_013854.csv\nFiles found for dataset 'productformula': ['data/incoming/productformula/productformula_20250127_013854.csv']\nValidating dataset: productformula, File: data/incoming/productformula/productformula_20250127_013854.csv\nFiles found for dataset 'sales': ['data/incoming/sales/sales_20250127_013854.csv']\nValidating dataset: sales, File: data/incoming/sales/sales_20250127_013854.csv\nFiles found for dataset 'supplier': ['data/incoming/supplier/supplier_20250127_013854.csv']\nValidating dataset: supplier, File: data/incoming/supplier/supplier_20250127_013854.csv\n            Dataset                                               File  \\\n0  customerfeedback  data/incoming/customerfeedback/customerfeedbac...   \n1  manufacturebatch  data/incoming/manufacturebatch/manufacturebatc...   \n2    productformula  data/incoming/productformula/productformula_20...   \n3             sales      data/incoming/sales/sales_20250127_013854.csv   \n4          supplier  data/incoming/supplier/supplier_20250127_01385...   \n\n   RecordCount  SchemaValid  \n0           50         True  \n1           50         True  \n2           20         True  \n3           50         True  \n4           20         True  \n"
     ]
    }
   ],
   "source": [
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "import pandas as pd\n",
    "\n",
    "# Azure Storage connection details\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=cdmo;AccountKey=XXXXXXXXXXXX\"\n",
    "container_name = \"00-landing\"\n",
    "storage_account_name = \"cdmo\"\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"LandingZoneValidation\").getOrCreate()\n",
    "\n",
    "# Set the Spark configuration for Azure Blob Storage\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# Initialize DataLakeServiceClient\n",
    "datalake_service_client = DataLakeServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Define dataset schemas for validation\n",
    "schemas = {\n",
    "    \"customerfeedback\": StructType([\n",
    "        StructField(\"FeedbackID\", StringType(), True),\n",
    "        StructField(\"ProductID\", StringType(), True),\n",
    "        StructField(\"CustomerID\", StringType(), True),\n",
    "        StructField(\"Rating\", IntegerType(), True),\n",
    "        StructField(\"Comments\", StringType(), True),\n",
    "        StructField(\"FeedbackDate\", StringType(), True)\n",
    "    ]),\n",
    "    \"manufacturebatch\": StructType([\n",
    "        StructField(\"BatchID\", StringType(), True),\n",
    "        StructField(\"ProductID\", StringType(), True),\n",
    "        StructField(\"BatchDate\", StringType(), True),\n",
    "        StructField(\"Quantity\", IntegerType(), True),\n",
    "        StructField(\"Status\", StringType(), True)\n",
    "    ]),\n",
    "    \"productformula\": StructType([\n",
    "        StructField(\"ProductID\", StringType(), True),\n",
    "        StructField(\"ProductName\", StringType(), True),\n",
    "        StructField(\"Category\", StringType(), True),\n",
    "        StructField(\"FormulationType\", StringType(), True),\n",
    "        StructField(\"PrimaryIngredients\", StringType(), True),\n",
    "        StructField(\"LaunchDate\", StringType(), True)\n",
    "    ]),\n",
    "    \"sales\": StructType([\n",
    "        StructField(\"OrderID\", StringType(), True),\n",
    "        StructField(\"CustomerID\", StringType(), True),\n",
    "        StructField(\"ProductID\", StringType(), True),\n",
    "        StructField(\"Quantity\", IntegerType(), True),\n",
    "        StructField(\"TotalAmount\", FloatType(), True),\n",
    "        StructField(\"OrderDate\", StringType(), True)\n",
    "    ]),\n",
    "    \"supplier\": StructType([\n",
    "        StructField(\"SupplierID\", StringType(), True),\n",
    "        StructField(\"SupplierName\", StringType(), True),\n",
    "        StructField(\"Material\", StringType(), True),\n",
    "        StructField(\"Cost\", FloatType(), True),\n",
    "        StructField(\"DeliveryDate\", StringType(), True)\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Function to list files in a directory\n",
    "def list_files_in_directory(container_name, directory_path):\n",
    "    try:\n",
    "        file_system_client = datalake_service_client.get_file_system_client(container_name)\n",
    "        paths = file_system_client.get_paths(path=directory_path)\n",
    "        files = [path.name for path in paths if not path.is_directory]\n",
    "        return files\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files in directory '{directory_path}': {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to validate a dataset\n",
    "def validate_dataset(dataset_name, schema, files):\n",
    "    validation_results = []\n",
    "    for file_path in files:\n",
    "        try:\n",
    "            # Construct the full file path in ADLS\n",
    "            full_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{file_path}\"\n",
    "            print(f\"Validating dataset: {dataset_name}, File: {file_path}\")\n",
    "\n",
    "            # Read the file into a DataFrame\n",
    "            df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(full_path)\n",
    "\n",
    "            # Record validation results\n",
    "            record_count = df.count()\n",
    "            validation_results.append({\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"File\": file_path,\n",
    "                \"RecordCount\": record_count,\n",
    "                \"SchemaValid\": True\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error validating dataset '{dataset_name}': {e}\")\n",
    "            validation_results.append({\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"File\": file_path,\n",
    "                \"RecordCount\": None,\n",
    "                \"SchemaValid\": False\n",
    "            })\n",
    "    return validation_results\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    all_validation_results = []\n",
    "\n",
    "    for dataset_name, schema in schemas.items():\n",
    "        directory_path = f\"data/incoming/{dataset_name}\"  # Landing zone path for each dataset\n",
    "        files = list_files_in_directory(container_name, directory_path)\n",
    "\n",
    "        if files:\n",
    "            print(f\"Files found for dataset '{dataset_name}': {files}\")\n",
    "            validation_results = validate_dataset(dataset_name, schema, files)\n",
    "            all_validation_results.extend(validation_results)\n",
    "        else:\n",
    "            print(f\"No files found for dataset '{dataset_name}' in landing zone.\")\n",
    "\n",
    "    # Convert the validation results to a DataFrame and display\n",
    "    validation_results_df = pd.DataFrame(all_validation_results)\n",
    "    print(validation_results_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-cdmo-datasets-creation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
