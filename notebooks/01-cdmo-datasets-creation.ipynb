{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fe84ea-a81f-4100-b68d-fa8a98fbf0f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pip install packages"
    }
   },
   "outputs": [],
   "source": [
    "pip install pyspark faker azure-storage-file-datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d0ceb8-a044-4c8f-9416-a648733cdaa0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import modules, libraries, classes, and functions"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f15f69e-4128-42a9-be5b-4a0b84aecd22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create cdmo daily datasets as csv"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ComprehensiveDataSets\").getOrCreate()\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Categories and formulation types\n",
    "categories = ['Foundation', 'Lipstick', 'Mascara', 'Eyeshadow', 'Blush']\n",
    "formulation_types = ['Liquid', 'Powder', 'Cream', 'Gel', 'Stick']\n",
    "primary_ingredients = ['Shea Butter', 'Hyaluronic Acid', 'Vitamin E', 'Collagen', 'Aloe Vera']\n",
    "status_options = ['Completed', 'In Progress', 'Failed', 'Pending']\n",
    "\n",
    "# Generate Product Formulations\n",
    "product_data = []\n",
    "product_ids = []  # To store ProductIDs for relationships\n",
    "for _ in range(20):\n",
    "    product_id = fake.uuid4()\n",
    "    product_ids.append(product_id)\n",
    "    product = {\n",
    "        \"ProductID\": product_id,\n",
    "        \"ProductName\": fake.word().capitalize() + \" \" + random.choice(categories),\n",
    "        \"Category\": random.choice(categories),\n",
    "        \"FormulationType\": random.choice(formulation_types),\n",
    "        \"PrimaryIngredients\": ', '.join(random.sample(primary_ingredients, 2)),\n",
    "        \"LaunchDate\": fake.date_between(start_date=\"-2y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    product_data.append(product)\n",
    "\n",
    "# Define schema for Product Formulations\n",
    "product_schema = StructType([\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"FormulationType\", StringType(), True),\n",
    "    StructField(\"PrimaryIngredients\", StringType(), True),\n",
    "    StructField(\"LaunchDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Product Formulations DataFrame\n",
    "product_formulations_df = spark.createDataFrame(product_data, product_schema)\n",
    "\n",
    "# Generate Manufacturing Batch Data\n",
    "batch_data = []\n",
    "for _ in range(50):\n",
    "    batch = {\n",
    "        \"BatchID\": fake.uuid4(),\n",
    "        \"ProductID\": random.choice(product_ids),\n",
    "        \"BatchDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\"),\n",
    "        \"Quantity\": random.randint(100, 1000),\n",
    "        \"Status\": random.choice(status_options)\n",
    "    }\n",
    "    batch_data.append(batch)\n",
    "\n",
    "# Define schema for Manufacturing Batches\n",
    "batch_schema = StructType([\n",
    "    StructField(\"BatchID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"BatchDate\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Manufacturing Batches DataFrame\n",
    "manufacturing_batch_df = spark.createDataFrame(batch_data, batch_schema)\n",
    "\n",
    "# Generate Customer Feedback Data\n",
    "customer_feedback_data = []\n",
    "customer_ids = [fake.uuid4() for _ in range(30)]  # Generate unique Customer IDs\n",
    "for _ in range(50):\n",
    "    feedback = {\n",
    "        \"FeedbackID\": fake.uuid4(),\n",
    "        \"ProductID\": random.choice(product_ids),\n",
    "        \"CustomerID\": random.choice(customer_ids),\n",
    "        \"Rating\": random.randint(1, 5),\n",
    "        \"Comments\": fake.sentence(),\n",
    "        \"FeedbackDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    customer_feedback_data.append(feedback)\n",
    "\n",
    "# Define schema for Customer Feedback\n",
    "customer_feedback_schema = StructType([\n",
    "    StructField(\"FeedbackID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"Rating\", IntegerType(), True),\n",
    "    StructField(\"Comments\", StringType(), True),\n",
    "    StructField(\"FeedbackDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Customer Feedback DataFrame\n",
    "customer_feedback_df = spark.createDataFrame(customer_feedback_data, customer_feedback_schema)\n",
    "\n",
    "# Generate Sales Data\n",
    "sales_data = []\n",
    "for _ in range(50):\n",
    "    sale = {\n",
    "        \"OrderID\": fake.uuid4(),\n",
    "        \"CustomerID\": random.choice(customer_ids),\n",
    "        \"ProductID\": random.choice(product_ids),\n",
    "        \"Quantity\": random.randint(1, 10),\n",
    "        \"TotalAmount\": round(random.uniform(20, 500), 2),\n",
    "        \"OrderDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    sales_data.append(sale)\n",
    "\n",
    "# Define schema for Sales Data\n",
    "sales_schema = StructType([\n",
    "    StructField(\"OrderID\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"TotalAmount\", FloatType(), True),\n",
    "    StructField(\"OrderDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Sales Data DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# Generate Supplier Information Data\n",
    "supplier_data = []\n",
    "supplier_ids = [fake.uuid4() for _ in range(10)]  # Generate unique Supplier IDs\n",
    "for _ in range(20):\n",
    "    supplier = {\n",
    "        \"SupplierID\": random.choice(supplier_ids),\n",
    "        \"SupplierName\": fake.company(),\n",
    "        \"Material\": random.choice(primary_ingredients),\n",
    "        \"Cost\": round(random.uniform(10, 100), 2),\n",
    "        \"DeliveryDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    supplier_data.append(supplier)\n",
    "\n",
    "# Define schema for Supplier Information\n",
    "supplier_schema = StructType([\n",
    "    StructField(\"SupplierID\", StringType(), True),\n",
    "    StructField(\"SupplierName\", StringType(), True),\n",
    "    StructField(\"Material\", StringType(), True),\n",
    "    StructField(\"Cost\", FloatType(), True),\n",
    "    StructField(\"DeliveryDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Supplier Information DataFrame\n",
    "supplier_df = spark.createDataFrame(supplier_data, supplier_schema)\n",
    "\n",
    "# Display the DataFrames\n",
    "# print(\"Product Formulations Dataset:\")\n",
    "# product_formulations_df.display()\n",
    "\n",
    "# print(\"Manufacturing Batches Dataset:\")\n",
    "# manufacturing_batch_df.display()\n",
    "\n",
    "# print(\"Customer Feedback Dataset:\")\n",
    "# customer_feedback_df.display()\n",
    "\n",
    "# print(\"Sales Dataset:\")\n",
    "# sales_df.display()\n",
    "\n",
    "# print(\"Supplier Information Dataset:\")\n",
    "# supplier_df.display()\n",
    "\n",
    "# Save DataFrames to CSV\n",
    "product_formulations_df.coalesce(1).write.csv(\"dbfs:/tmp/product_formulations.csv\", header=True, mode=\"overwrite\")\n",
    "manufacturing_batch_df.coalesce(1).write.csv(\"dbfs:/tmp/manufacturing_batches.csv\", header=True, mode=\"overwrite\")\n",
    "customer_feedback_df.coalesce(1).write.csv(\"dbfs:/tmp/customer_feedback.csv\", header=True, mode=\"overwrite\")\n",
    "sales_df.coalesce(1).write.csv(\"dbfs:/tmp/sales_data.csv\", header=True, mode=\"overwrite\")\n",
    "supplier_df.coalesce(1).write.csv(\"dbfs:/tmp/supplier_information.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41e487f-812b-40aa-b0c4-c4b6d9ac9d72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Upload csv datasets to landing zone"
    }
   },
   "outputs": [],
   "source": [
    "# Azure Storage connection details\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=cdmo;AccountKey=XXXXXXXXX\"\n",
    "container_name = \"00-landing\"\n",
    "\n",
    "# Function to upload a file to ADLS\n",
    "def upload_to_adls(local_path, blob_path):\n",
    "    try:\n",
    "        # Initialize BlobServiceClient\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "        \n",
    "        # Get the blob client for the target file\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)\n",
    "        \n",
    "        # Upload the file\n",
    "        with open(local_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "        print(f\"Uploaded file to ADLS: {blob_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to ADLS: {e}\")\n",
    "\n",
    "# Define datasets and their corresponding ADLS folders\n",
    "datasets = {\n",
    "    \"customerfeedback\": customer_feedback_df,\n",
    "    \"manufacturebatch\": manufacturing_batch_df,\n",
    "    \"productformula\": product_formulations_df,\n",
    "    \"sales\": sales_df,\n",
    "    \"supplier\": supplier_df,\n",
    "}\n",
    "\n",
    "# Get the current date and time\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "current_time = datetime.now().strftime(\"%H%M%S\")\n",
    "\n",
    "# Save DataFrames to DBFS and upload to ADLS\n",
    "for dataset_name, dataframe in datasets.items():\n",
    "    try:\n",
    "        # Define the DBFS and local paths\n",
    "        dbfs_temp_dir = f\"dbfs:/tmp/{dataset_name}\"  # Temporary directory for Spark output\n",
    "        local_temp_dir = f\"/dbfs/tmp/{dataset_name}\"  # Local path for accessing DBFS files\n",
    "        os.makedirs(local_temp_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the DataFrame to DBFS as a single CSV\n",
    "        dataframe.coalesce(1).write.csv(dbfs_temp_dir, header=True, mode=\"overwrite\")\n",
    "        \n",
    "        # Locate the part file in the DBFS directory\n",
    "        files = dbutils.fs.ls(dbfs_temp_dir)\n",
    "        part_file = next(f.path for f in files if f.name.startswith(\"part-\"))\n",
    "        \n",
    "        # Rename the part file to a meaningful name with date and time\n",
    "        local_file_path = os.path.join(local_temp_dir, f\"{dataset_name}_{current_date}_{current_time}.csv\")\n",
    "        dbutils.fs.cp(part_file, f\"file:{local_file_path}\")\n",
    "        \n",
    "        # Define the blob path in ADLS with the date folder and timestamped file name\n",
    "        adls_incoming_blob_path = f\"data/incoming/{dataset_name}/{dataset_name}_{current_date}_{current_time}.csv\"\n",
    "        \n",
    "        # Upload the file to ADLS\n",
    "        upload_to_adls(local_file_path, adls_incoming_blob_path)\n",
    "        \n",
    "        # Clean up temporary directories\n",
    "        dbutils.fs.rm(dbfs_temp_dir, recurse=True)\n",
    "        print(f\"Dataset '{dataset_name}' successfully uploaded to ADLS as {adls_blob_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset '{dataset_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695ce2a3-946f-40cd-8f58-8b408fd635d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validate Datasets"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the specific file\n",
    "file_path = \"dbfs:/tmp/product_formulations.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-cdmo-datasets-creation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
