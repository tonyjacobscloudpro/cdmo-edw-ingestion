{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676d7eaf-e1ff-4662-9929-8578598ec0d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pip install necessary packages"
    }
   },
   "outputs": [],
   "source": [
    "pip install azure-storage-blob azure-storage-file-datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc48cbb3-301b-438d-96c3-ac839f8a5d15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Merge silver into gold delta tables"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, trim, col, lit, concat_ws, expr\n",
    "from io import StringIO\n",
    "\n",
    "# Azure Storage connection details\n",
    "storage_account_name = \"cdmo\"\n",
    "storage_account_key = \"XXXXXXXXXXXXXXX\"\n",
    "container_name = \"config\"\n",
    "metadata_file_name = \"metadata_config_20250127.csv\"\n",
    "\n",
    "# Set Spark configuration for Azure Blob Storage\n",
    "spark = SparkSession.builder.appName(\"GoldLayerProcessing\").getOrCreate()\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# Initialize BlobServiceClient\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key}\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Load Metadata File from ADLS\n",
    "def load_metadata():\n",
    "    try:\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=metadata_file_name)\n",
    "\n",
    "        # Download the metadata content\n",
    "        metadata_content = blob_client.download_blob().readall().decode(\"utf-8\")\n",
    "        print(f\"Successfully downloaded metadata file: {metadata_file_name}\")\n",
    "\n",
    "        # Parse CSV content and filter for 'Gold' layer\n",
    "        csv_reader = csv.DictReader(StringIO(metadata_content))\n",
    "        metadata_list = [row for row in csv_reader if row[\"Layer\"].strip().lower() == \"gold\"]\n",
    "\n",
    "        print(f\"Filtered metadata for 'Gold' layer: {len(metadata_list)} entries found.\")\n",
    "        return metadata_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata file: {e}\")\n",
    "        raise\n",
    "\n",
    "# Apply transformations specific to the Gold Layer\n",
    "def transform_gold_layer(source_df, dataset_name):\n",
    "    try:\n",
    "        # Common transformations for all datasets\n",
    "        source_df = source_df.withColumn(\"ProcessedTimestamp\", current_timestamp())\n",
    "\n",
    "        # Dataset-specific transformations\n",
    "        if dataset_name == \"customerfeedback\":\n",
    "            source_df = source_df.withColumn(\"Sentiment\", expr(\"CASE WHEN Rating >= 4 THEN 'Positive' ELSE 'Negative' END\"))\n",
    "\n",
    "        elif dataset_name == \"manufacturebatch\":\n",
    "            source_df = source_df.withColumn(\"BatchStatus\", expr(\"CASE WHEN Status = 'Completed' THEN 'Closed' ELSE 'Open' END\"))\n",
    "\n",
    "        elif dataset_name == \"productformula\":\n",
    "            source_df = source_df.withColumn(\"PrimaryIngredientList\", concat_ws(\", \", col(\"PrimaryIngredients\")))\n",
    "\n",
    "        elif dataset_name == \"sales\":\n",
    "            source_df = source_df.withColumn(\"TotalRevenue\", expr(\"Quantity * TotalAmount\"))\n",
    "\n",
    "        elif dataset_name == \"supplier\":\n",
    "            source_df = source_df.withColumn(\"SupplierDetails\", concat_ws(\" - \", col(\"SupplierName\"), col(\"Material\")))\n",
    "\n",
    "        print(f\"Gold layer transformations applied successfully for dataset: {dataset_name}\")\n",
    "        return source_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gold layer transformations: {e}\")\n",
    "        raise\n",
    "\n",
    "# Perform Delta Merge into Gold Layer\n",
    "def merge_into_gold_layer(source_df, target_path, unique_key):\n",
    "    try:\n",
    "        # Read the target Gold Delta table\n",
    "        if spark._jsparkSession.catalog().tableExists(f\"delta.`{target_path}`\"):\n",
    "            target_df = spark.read.format(\"delta\").load(target_path)\n",
    "        else:\n",
    "            print(f\"Target path {target_path} does not exist. Writing as new table.\")\n",
    "            source_df.write.format(\"delta\").mode(\"overwrite\").save(target_path)\n",
    "            return\n",
    "\n",
    "        # Register source and target DataFrames as temporary views for SQL-based merge\n",
    "        source_df.createOrReplaceTempView(\"source_temp_view\")\n",
    "        target_df.createOrReplaceTempView(\"target_temp_view\")\n",
    "\n",
    "        # Merge query to update existing records and insert new ones\n",
    "        merge_query = f\"\"\"\n",
    "        MERGE INTO delta.`{target_path}` AS target\n",
    "        USING source_temp_view AS source\n",
    "        ON target.{unique_key} = source.{unique_key}\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query)\n",
    "        print(f\"Data successfully merged into Gold Layer at {target_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Delta Merge: {e}\")\n",
    "        raise\n",
    "\n",
    "# Process Data Based on Metadata\n",
    "def process_data(metadata_list):\n",
    "    for metadata in metadata_list:\n",
    "        try:\n",
    "            # Extract metadata details\n",
    "            source_container = metadata[\"SourceContainer\"]\n",
    "            source_path = metadata[\"SourcePath\"]\n",
    "            source_format = metadata[\"SourceFormat\"]\n",
    "            target_container = metadata[\"TargetContainer\"]\n",
    "            target_path = metadata[\"TargetPath\"]\n",
    "            unique_key = metadata.get(\"UniqueKey\")  # Use get() to handle missing keys\n",
    "            dataset_name = metadata.get(\"DatasetName\")\n",
    "\n",
    "            # Validate UniqueKey\n",
    "            if not unique_key:\n",
    "                print(f\"Skipping dataset {source_path}: 'UniqueKey' not provided in metadata.\")\n",
    "                continue\n",
    "\n",
    "            # Construct full paths\n",
    "            source_full_path = f\"wasbs://{source_container}@{storage_account_name}.blob.core.windows.net/{source_path}\"\n",
    "            target_full_path = f\"wasbs://{target_container}@{storage_account_name}.blob.core.windows.net/{target_path}\"\n",
    "\n",
    "            print(f\"Processing: {source_full_path} -> {target_full_path}\")\n",
    "\n",
    "            # Read source data in Delta format\n",
    "            source_df = spark.read.format(\"delta\").load(source_full_path)\n",
    "\n",
    "            # Apply Gold Layer transformations\n",
    "            gold_df = transform_gold_layer(source_df, dataset_name)\n",
    "\n",
    "            # Perform Delta Merge\n",
    "            merge_into_gold_layer(gold_df, target_full_path, unique_key)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data for Source: {metadata['SourcePath']}: {e}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load metadata filtered for 'Gold' layer\n",
    "    metadata_list = load_metadata()\n",
    "\n",
    "    # Process data based on metadata\n",
    "    process_data(metadata_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b877248-d3ef-42a4-93fa-b0da961f4b2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validation 1"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Azure Storage connection details\n",
    "storage_account_name = \"cdmo\"\n",
    "storage_account_key = \"XXXXXXXXXXXXXXX\"\n",
    "container_name = \"03-gold\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"GoldLayerValidation\").getOrCreate()\n",
    "\n",
    "# Set the Spark configuration for Azure Blob Storage\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# List of tables in the gold layer\n",
    "tables = [\n",
    "    \"customer_feedback_aggregated\",\n",
    "    \"manufacture_batch_enriched\",\n",
    "    \"product_formula_enriched\",\n",
    "    \"sales_aggregated\",\n",
    "    \"supplier_data_enriched\"\n",
    "]\n",
    "\n",
    "# Validate table counts and sample data\n",
    "def validate_gold_tables(container_name, tables):\n",
    "    try:\n",
    "        for table in tables:\n",
    "            # Construct the full path for each table in the gold layer\n",
    "            table_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{table}/\"\n",
    "\n",
    "            print(f\"Validating table: {table} at {table_path}\")\n",
    "            \n",
    "            # Read the table into a DataFrame\n",
    "            try:\n",
    "                df = spark.read.format(\"delta\").load(table_path)\n",
    "\n",
    "                # Get the count of records\n",
    "                record_count = df.count()\n",
    "                print(f\"Table: {table}, Record Count: {record_count}\")\n",
    "\n",
    "                # Display a sample of the data\n",
    "                print(f\"Sample data for table: {table}\")\n",
    "                df.show(5, truncate=False)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading table '{table}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating gold tables: {e}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    validate_gold_tables(container_name, tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a2cdbff-7c8c-4dab-b0e7-6afd325a7ca3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validation 2"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Azure Storage connection details\n",
    "storage_account_name = \"cdmo\"\n",
    "storage_account_key = \"XXXXXXXXXXXXXXX\"\n",
    "container_name = \"03-gold\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"GoldLayerValidation\").getOrCreate()\n",
    "\n",
    "# Set the Spark configuration for Azure Blob Storage\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# List of tables in the Gold Layer\n",
    "tables = {\n",
    "    \"customer_feedback_aggregated\": \"FeedbackID\",\n",
    "    \"manufacture_batch_enriched\": \"BatchID\",\n",
    "    \"product_formula_enriched\": \"ProductID\",\n",
    "    \"sales_aggregated\": \"OrderID\",\n",
    "    \"supplier_data_enriched\": \"SupplierID\"\n",
    "}\n",
    "\n",
    "# Validate table counts and sample data\n",
    "def validate_gold_tables(container_name, tables):\n",
    "    try:\n",
    "        for table, primary_key in tables.items():\n",
    "            # Construct the full path for each table in the Gold Layer\n",
    "            table_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{table}/\"\n",
    "\n",
    "            print(f\"\\nValidating table: {table} at {table_path}\")\n",
    "            \n",
    "            # Read the table into a DataFrame\n",
    "            try:\n",
    "                df = spark.read.format(\"delta\").load(table_path)\n",
    "\n",
    "                # Get the count of records\n",
    "                record_count = df.count()\n",
    "                print(f\"Table: {table}, Record Count: {record_count}\")\n",
    "\n",
    "                # Check for duplicate primary keys\n",
    "                duplicate_count = df.groupBy(primary_key).count().filter(\"count > 1\").count()\n",
    "                print(f\"Table: {table}, Duplicate Primary Key Records: {duplicate_count}\")\n",
    "\n",
    "                # Display a sample of the data\n",
    "                print(f\"Sample data for table: {table}\")\n",
    "                df.display()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading table '{table}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating gold tables: {e}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    validate_gold_tables(container_name, tables)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05-load-gold-layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
