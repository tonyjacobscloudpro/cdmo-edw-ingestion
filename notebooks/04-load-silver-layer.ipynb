{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676d7eaf-e1ff-4662-9929-8578598ec0d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pip install necessary packages"
    }
   },
   "outputs": [],
   "source": [
    "pip install azure-storage-blob azure-storage-file-datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc48cbb3-301b-438d-96c3-ac839f8a5d15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Merge bronze into silver delta tables"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, trim, input_file_name\n",
    "from io import StringIO\n",
    "\n",
    "# Azure Storage connection details\n",
    "storage_account_name = \"cdmo\"\n",
    "storage_account_key = \"XXXXXXXXXXXX\"\n",
    "container_name = \"config\"\n",
    "metadata_file_name = \"metadata_config_20250127.csv\"\n",
    "\n",
    "# Set Spark configuration for Azure Blob Storage\n",
    "spark = SparkSession.builder.appName(\"SilverLayerProcessing\").getOrCreate()\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# Initialize BlobServiceClient\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key}\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Load Metadata File from ADLS\n",
    "def load_metadata():\n",
    "    try:\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=metadata_file_name)\n",
    "\n",
    "        # Download the metadata content\n",
    "        metadata_content = blob_client.download_blob().readall().decode(\"utf-8\")\n",
    "        print(f\"Successfully downloaded metadata file: {metadata_file_name}\")\n",
    "\n",
    "        # Parse CSV content and filter for 'Silver' layer\n",
    "        csv_reader = csv.DictReader(StringIO(metadata_content))\n",
    "        metadata_list = [row for row in csv_reader if row[\"Layer\"].strip().lower() == \"silver\"]\n",
    "\n",
    "        print(f\"Filtered metadata for 'Silver' layer: {len(metadata_list)} entries found.\")\n",
    "        return metadata_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata file: {e}\")\n",
    "        raise\n",
    "\n",
    "# Check if files exist in the source path\n",
    "def files_exist_in_path(container, source_path):\n",
    "    try:\n",
    "        file_system_client = blob_service_client.get_container_client(container)\n",
    "        paths = file_system_client.walk_blobs(name_starts_with=source_path)\n",
    "        files = [blob.name for blob in paths]\n",
    "        return files if files else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking files in path '{source_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply transformations specific to the Silver Layer\n",
    "def transform_silver_layer(source_df):\n",
    "    try:\n",
    "        # Example transformations:\n",
    "        string_cols = [col_name for col_name, dtype in source_df.dtypes if dtype == \"string\"]\n",
    "        for col_name in string_cols:\n",
    "            source_df = source_df.withColumn(col_name, trim(source_df[col_name]))\n",
    "        \n",
    "        source_df = source_df.withColumn(\"ProcessedTimestamp\", current_timestamp())\n",
    "        print(\"Silver layer transformations applied successfully.\")\n",
    "        return source_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Silver layer transformations: {e}\")\n",
    "        raise\n",
    "\n",
    "# Perform Delta Merge into Silver Layer\n",
    "def merge_into_silver_layer(source_df, target_path, unique_key):\n",
    "    try:\n",
    "        # Read the target Silver Delta table\n",
    "        if spark._jsparkSession.catalog().tableExists(f\"delta.`{target_path}`\"):\n",
    "            target_df = spark.read.format(\"delta\").load(target_path)\n",
    "        else:\n",
    "            print(f\"Target path {target_path} does not exist. Writing as new table.\")\n",
    "            source_df.write.format(\"delta\").mode(\"overwrite\").save(target_path)\n",
    "            return\n",
    "\n",
    "        # Register source and target DataFrames as temporary views for SQL-based merge\n",
    "        source_df.createOrReplaceTempView(\"source_temp_view\")\n",
    "        target_df.createOrReplaceTempView(\"target_temp_view\")\n",
    "\n",
    "        # Merge query to update existing records and insert new ones\n",
    "        merge_query = f\"\"\"\n",
    "        MERGE INTO delta.`{target_path}` AS target\n",
    "        USING source_temp_view AS source\n",
    "        ON target.{unique_key} = source.{unique_key}\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query)\n",
    "        print(f\"Data successfully merged into Silver Layer at {target_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Delta Merge: {e}\")\n",
    "        raise\n",
    "\n",
    "# Process Data Based on Metadata\n",
    "def process_data(metadata_list):\n",
    "    for metadata in metadata_list:\n",
    "        try:\n",
    "            # Extract metadata details\n",
    "            source_container = metadata[\"SourceContainer\"]\n",
    "            source_path = metadata[\"SourcePath\"]\n",
    "            source_format = metadata[\"SourceFormat\"]\n",
    "            target_container = metadata[\"TargetContainer\"]\n",
    "            target_path = metadata[\"TargetPath\"]\n",
    "            unique_key = metadata.get(\"UniqueKey\")  # Use get() to handle missing keys\n",
    "            add_timestamp = metadata[\"AddTimestamp\"].lower() == \"true\"\n",
    "\n",
    "            # Validate UniqueKey\n",
    "            if not unique_key:\n",
    "                print(f\"Skipping dataset {source_path}: 'UniqueKey' not provided in metadata.\")\n",
    "                continue\n",
    "\n",
    "            # Check if files exist in the source path\n",
    "            files = files_exist_in_path(source_container, source_path)\n",
    "            if not files:\n",
    "                print(f\"No files found in source path: {source_path}. Skipping dataset.\")\n",
    "                continue\n",
    "\n",
    "            # Construct full paths\n",
    "            source_full_path = f\"wasbs://{source_container}@{storage_account_name}.blob.core.windows.net/{source_path}\"\n",
    "            target_full_path = f\"wasbs://{target_container}@{storage_account_name}.blob.core.windows.net/{target_path}\"\n",
    "\n",
    "            print(f\"Processing: {source_full_path} -> {target_full_path}\")\n",
    "\n",
    "            # Read source data in Delta format\n",
    "            source_df = spark.read.format(\"delta\").load(source_full_path)\n",
    "\n",
    "            # Apply Silver Layer transformations\n",
    "            silver_df = transform_silver_layer(source_df)\n",
    "\n",
    "            # Perform Delta Merge\n",
    "            merge_into_silver_layer(silver_df, target_full_path, unique_key)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data for Source: {metadata['SourcePath']}: {e}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load metadata filtered for 'Silver' layer\n",
    "    metadata_list = load_metadata()\n",
    "\n",
    "    # Process data based on metadata\n",
    "    process_data(metadata_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b877248-d3ef-42a4-93fa-b0da961f4b2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validation 1"
    }
   },
   "outputs": [],
   "source": [
    "container_name = \"02-silver\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SilverLayerValidation\").getOrCreate()\n",
    "\n",
    "# Set the Spark configuration for Azure Blob Storage\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# List of tables in the bronze layer\n",
    "tables = [\n",
    "    \"customerfeedback\",\n",
    "    \"manufacturebatch\",\n",
    "    \"productformula\",\n",
    "    \"sales\",\n",
    "    \"supplier\"\n",
    "]\n",
    "\n",
    "# Validate table counts\n",
    "def validate_silver_tables(container_name, tables):\n",
    "    try:\n",
    "        for table in tables:\n",
    "            # Construct the full path for each table in the bronze layer\n",
    "            table_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{table}/\"\n",
    "\n",
    "            print(f\"Validating table: {table} at {table_path}\")\n",
    "            \n",
    "            # Read the table into a DataFrame\n",
    "            try:\n",
    "                df = spark.read.format(\"delta\").load(table_path)\n",
    "\n",
    "                # Get the count of records\n",
    "                record_count = df.count()\n",
    "                print(f\"Table: {table}, Record Count: {record_count}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading table '{table}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating silver tables: {e}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    validate_silver_tables(container_name, tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a2cdbff-7c8c-4dab-b0e7-6afd325a7ca3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validation 2"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Azure Storage connection details\n",
    "storage_account_name = \"cdmo\"\n",
    "storage_account_key = \"XXXXXXXXXXXXXXXX\"\n",
    "container_name = \"02-silver\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SilverLayerValidation\").getOrCreate()\n",
    "\n",
    "# Set the Spark configuration for Azure Blob Storage\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# List of tables in the Bronze layer\n",
    "tables = [\n",
    "    \"customerfeedback\",\n",
    "    \"manufacturebatch\",\n",
    "    \"productformula\",\n",
    "    \"sales\",\n",
    "    \"supplier\"\n",
    "]\n",
    "\n",
    "# Validate table counts and display records\n",
    "def validate_silver_tables(container_name, tables):\n",
    "    try:\n",
    "        for table in tables:\n",
    "            # Construct the full path for each table in the Bronze layer\n",
    "            table_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{table}/\"\n",
    "\n",
    "            print(f\"Validating table: {table} at {table_path}\")\n",
    "\n",
    "            # Read the table into a DataFrame\n",
    "            try:\n",
    "                df = spark.read.format(\"delta\").load(table_path)\n",
    "\n",
    "                # Get the count of records\n",
    "                record_count = df.count()\n",
    "                print(f\"Table: {table}, Record Count: {record_count}\")\n",
    "\n",
    "                # Display the first 5 records for validation\n",
    "                print(f\"Displaying the first 5 records for table: {table}\")\n",
    "                df.display()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading table '{table}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating silver tables: {e}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    validate_silver_tables(container_name, tables)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-load-silver-layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
