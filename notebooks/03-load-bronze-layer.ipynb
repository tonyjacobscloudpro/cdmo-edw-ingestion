{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676d7eaf-e1ff-4662-9929-8578598ec0d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pip install necessary packages"
    }
   },
   "outputs": [],
   "source": [
    "pip install azure-storage-blob azure-storage-file-datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc48cbb3-301b-438d-96c3-ac839f8a5d15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Append landing data to bronze delta tables"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, lit, input_file_name\n",
    "from io import StringIO\n",
    "\n",
    "# Azure Storage connection details\n",
    "storage_account_name = \"cdmo\"\n",
    "storage_account_key = \"XXXXXXXXXXXXXXXX\"\n",
    "container_name = \"config\"\n",
    "metadata_file_name = \"metadata_config_20250127.csv\"\n",
    "\n",
    "# Set Spark configuration for Azure Blob Storage\n",
    "spark = SparkSession.builder.appName(\"MetadataDrivenLoads\").getOrCreate()\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# Initialize BlobServiceClient\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key}\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Load Metadata File from ADLS\n",
    "def load_metadata():\n",
    "    try:\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=metadata_file_name)\n",
    "\n",
    "        # Download the metadata content\n",
    "        metadata_content = blob_client.download_blob().readall().decode(\"utf-8\")\n",
    "        print(f\"Successfully downloaded metadata file: {metadata_file_name}\")\n",
    "\n",
    "        # Parse CSV content and filter for 'Bronze' layer\n",
    "        csv_reader = csv.DictReader(StringIO(metadata_content))\n",
    "        metadata_list = [row for row in csv_reader if row[\"Layer\"].strip().lower() == \"bronze\"]\n",
    "\n",
    "        print(f\"Filtered metadata for 'Bronze' layer: {len(metadata_list)} entries found.\")\n",
    "        return metadata_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata file: {e}\")\n",
    "        raise\n",
    "\n",
    "# Check if files exist in the source path\n",
    "def files_exist_in_path(container, source_path):\n",
    "    try:\n",
    "        file_system_client = blob_service_client.get_container_client(container)\n",
    "        paths = file_system_client.walk_blobs(name_starts_with=source_path)\n",
    "        files = [blob.name for blob in paths]\n",
    "        return files if files else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking files in path '{source_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Process Data Based on Metadata\n",
    "def process_data(metadata_list):\n",
    "    for metadata in metadata_list:\n",
    "        try:\n",
    "            # Extract metadata details\n",
    "            source_container = metadata[\"SourceContainer\"]\n",
    "            source_path = metadata[\"SourcePath\"]\n",
    "            source_format = metadata[\"SourceFormat\"]\n",
    "            target_container = metadata[\"TargetContainer\"]\n",
    "            target_path = metadata[\"TargetPath\"]\n",
    "            target_format = metadata[\"TargetFormat\"]\n",
    "            add_timestamp = metadata[\"AddTimestamp\"].lower() == \"true\"\n",
    "\n",
    "            # Check if files exist in the source path\n",
    "            files = files_exist_in_path(source_container, source_path)\n",
    "            if not files:\n",
    "                print(f\"No files found in source path: {source_path}. Skipping dataset.\")\n",
    "                continue\n",
    "\n",
    "            # Construct full paths\n",
    "            source_full_path = f\"wasbs://{source_container}@{storage_account_name}.blob.core.windows.net/{source_path}\"\n",
    "            target_full_path = f\"wasbs://{target_container}@{storage_account_name}.blob.core.windows.net/{target_path}\"\n",
    "\n",
    "            print(f\"Processing: {source_full_path} -> {target_full_path}\")\n",
    "\n",
    "            # Read source data and include filename in each row\n",
    "            source_df = (\n",
    "                spark.read.format(source_format)\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(source_full_path)\n",
    "                .withColumn(\"Filename\", input_file_name())  # Add filename column\n",
    "            )\n",
    "\n",
    "            # Apply transformations if specified\n",
    "            if add_timestamp:\n",
    "                source_df = source_df.withColumn(\"LoadTimestamp\", current_timestamp())\n",
    "\n",
    "            # Save to target\n",
    "            source_df.write.format(target_format).mode(\"append\").save(target_full_path)\n",
    "            print(f\"Data successfully saved to {target_full_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data for Source: {metadata['SourcePath']}: {e}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load metadata filtered for 'Bronze' layer\n",
    "    metadata_list = load_metadata()\n",
    "\n",
    "    # Process data based on metadata\n",
    "    process_data(metadata_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b48066-3def-42e7-96ed-3b3966d4c428",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Move csv files to archive"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "import os\n",
    "\n",
    "# Azure Storage connection details\n",
    "account_name = \"cdmo\"\n",
    "account_key = \"XXXXXXXXXXXXXX\"\n",
    "file_system_name = \"00-landing\"\n",
    "\n",
    "# Initialize DataLakeServiceClient\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{account_name}.dfs.core.windows.net\",\n",
    "    credential=account_key,\n",
    ")\n",
    "\n",
    "# Function to list all files in a directory recursively\n",
    "def list_files(service_client, file_system_name, directory_path):\n",
    "    try:\n",
    "        file_system_client = service_client.get_file_system_client(file_system_name)\n",
    "        paths = file_system_client.get_paths(path=directory_path)\n",
    "        files = [path.name for path in paths if not path.is_directory]\n",
    "        return files\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files in directory '{directory_path}': {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to copy a file from source to target\n",
    "def copy_file(service_client, file_system_name, source_path, target_path):\n",
    "    try:\n",
    "        file_system_client = service_client.get_file_system_client(file_system_name)\n",
    "        source_file_client = file_system_client.get_file_client(source_path)\n",
    "        target_file_client = file_system_client.get_file_client(target_path)\n",
    "\n",
    "        # Start copy operation\n",
    "        copy_operation = target_file_client.start_copy_from_url(source_file_client.url)\n",
    "        print(f\"Copy initiated from '{source_path}' to '{target_path}'\")\n",
    "\n",
    "        # Wait for copy operation to complete\n",
    "        props = target_file_client.get_file_properties()\n",
    "        while props.copy.status == \"pending\":\n",
    "            props = target_file_client.get_file_properties()\n",
    "        print(f\"File copied successfully to '{target_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying file from '{source_path}' to '{target_path}': {e}\")\n",
    "\n",
    "# Function to delete a file after it is copied\n",
    "def delete_file(service_client, file_system_name, file_path):\n",
    "    try:\n",
    "        file_system_client = service_client.get_file_system_client(file_system_name)\n",
    "        file_client = file_system_client.get_file_client(file_path)\n",
    "        file_client.delete_file()\n",
    "        print(f\"Deleted file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting file '{file_path}': {e}\")\n",
    "\n",
    "# Function to move files from incoming to archive\n",
    "def move_files_to_archive(service_client, file_system_name, source_base_dir, target_base_dir):\n",
    "    try:\n",
    "        files = list_files(service_client, file_system_name, source_base_dir)\n",
    "\n",
    "        if not files:\n",
    "            print(f\"No files found in the source directory: {source_base_dir}\")\n",
    "            return\n",
    "\n",
    "        for file in files:\n",
    "            # Generate source and target paths\n",
    "            relative_path = file.replace(source_base_dir, \"\").lstrip(\"/\")\n",
    "            source_path = file\n",
    "            target_path = os.path.join(target_base_dir, relative_path)\n",
    "\n",
    "            # Copy the file\n",
    "            copy_file(service_client, file_system_name, source_path, target_path)\n",
    "\n",
    "            # Delete the original file\n",
    "            delete_file(service_client, file_system_name, source_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving files from '{source_base_dir}' to '{target_base_dir}': {e}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    source_base_dir = \"data/incoming\"\n",
    "    target_base_dir = \"data/archive\"\n",
    "\n",
    "    move_files_to_archive(service_client, file_system_name, source_base_dir, target_base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b877248-d3ef-42a4-93fa-b0da961f4b2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validation 1"
    }
   },
   "outputs": [],
   "source": [
    "container_name = \"01-bronze\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BronzeLayerValidation\").getOrCreate()\n",
    "\n",
    "# Set the Spark configuration for Azure Blob Storage\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# List of tables in the bronze layer\n",
    "tables = [\n",
    "    \"customerfeedback\",\n",
    "    \"manufacturebatch\",\n",
    "    \"productformula\",\n",
    "    \"sales\",\n",
    "    \"supplier\"\n",
    "]\n",
    "\n",
    "# Validate table counts\n",
    "def validate_bronze_tables(container_name, tables):\n",
    "    try:\n",
    "        for table in tables:\n",
    "            # Construct the full path for each table in the bronze layer\n",
    "            table_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{table}/\"\n",
    "\n",
    "            print(f\"Validating table: {table} at {table_path}\")\n",
    "            \n",
    "            # Read the table into a DataFrame\n",
    "            try:\n",
    "                df = spark.read.format(\"delta\").load(table_path)\n",
    "\n",
    "                # Get the count of records\n",
    "                record_count = df.count()\n",
    "                print(f\"Table: {table}, Record Count: {record_count}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading table '{table}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating bronze tables: {e}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    validate_bronze_tables(container_name, tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a2cdbff-7c8c-4dab-b0e7-6afd325a7ca3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validation 2"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Azure Storage connection details\n",
    "storage_account_name = \"cdmo\"\n",
    "storage_account_key = \"XXXXXXXXXXXX\"\n",
    "container_name = \"01-bronze\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BronzeLayerValidation\").getOrCreate()\n",
    "\n",
    "# Set the Spark configuration for Azure Blob Storage\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# List of tables in the Bronze layer\n",
    "tables = [\n",
    "    \"customerfeedback\",\n",
    "    \"manufacturebatch\",\n",
    "    \"productformula\",\n",
    "    \"sales\",\n",
    "    \"supplier\"\n",
    "]\n",
    "\n",
    "# Validate table counts and display records\n",
    "def validate_bronze_tables(container_name, tables):\n",
    "    try:\n",
    "        for table in tables:\n",
    "            # Construct the full path for each table in the Bronze layer\n",
    "            table_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{table}/\"\n",
    "\n",
    "            print(f\"Validating table: {table} at {table_path}\")\n",
    "\n",
    "            # Read the table into a DataFrame\n",
    "            try:\n",
    "                df = spark.read.format(\"delta\").load(table_path)\n",
    "\n",
    "                # Get the count of records\n",
    "                record_count = df.count()\n",
    "                print(f\"Table: {table}, Record Count: {record_count}\")\n",
    "\n",
    "                # Display the first 5 records for validation\n",
    "                print(f\"Displaying the first 5 records for table: {table}\")\n",
    "                df.display(5)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading table '{table}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating bronze tables: {e}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    validate_bronze_tables(container_name, tables)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03-load-bronze-layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
