{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fe84ea-a81f-4100-b68d-fa8a98fbf0f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pip install packages"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting pyspark\n  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\nCollecting faker\n  Downloading Faker-35.0.0-py3-none-any.whl (1.9 MB)\nCollecting azure-storage-file-datalake\n  Using cached azure_storage_file_datalake-12.18.1-py3-none-any.whl (258 kB)\nCollecting py4j==0.10.9.7\n  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\nRequirement already satisfied: python-dateutil>=2.4 in /databricks/python3/lib/python3.9/site-packages (from faker) (2.8.2)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.9/site-packages (from faker) (4.1.1)\nCollecting azure-storage-blob>=12.24.1\n  Using cached azure_storage_blob-12.24.1-py3-none-any.whl (408 kB)\nCollecting azure-core>=1.30.0\n  Using cached azure_core-1.32.0-py3-none-any.whl (198 kB)\nCollecting typing-extensions\n  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nCollecting isodate>=0.6.1\n  Using cached isodate-0.7.2-py3-none-any.whl (22 kB)\nRequirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core>=1.30.0->azure-storage-file-datalake) (1.16.0)\nRequirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core>=1.30.0->azure-storage-file-datalake) (2.27.1)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.9/site-packages (from azure-storage-blob>=12.24.1->azure-storage-file-datalake) (3.4.8)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.9/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.24.1->azure-storage-file-datalake) (1.15.0)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.24.1->azure-storage-file-datalake) (2.21)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (1.26.9)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py): started\n  Building wheel for pyspark (setup.py): finished with status 'done'\n  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849792 sha256=605c8cb0bc6deeff255eb8991b8706e9c4833e85ac10cad55a7b4a4640a6f33f\n  Stored in directory: /root/.cache/pip/wheels/b9/0f/0a/1bf9096f5b49f278182d7fe905a82209f2090edb24a7352b72\nSuccessfully built pyspark\nInstalling collected packages: typing-extensions, isodate, azure-core, py4j, azure-storage-blob, pyspark, faker, azure-storage-file-datalake\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2dbb161e-e0d5-42d0-b744-9eba86692dc2\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\nSuccessfully installed azure-core-1.32.0 azure-storage-blob-12.24.1 azure-storage-file-datalake-12.18.1 faker-35.0.0 isodate-0.7.2 py4j-0.10.9.7 pyspark-3.5.4 typing-extensions-4.12.2\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark faker azure-storage-file-datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d0ceb8-a044-4c8f-9416-a648733cdaa0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import modules, libraries, classes, and functions"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f15f69e-4128-42a9-be5b-4a0b84aecd22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create cdmo daily datasets as csv"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ComprehensiveDataSets\").getOrCreate()\n",
    "\n",
    "# Initialize Faker for generating mock data\n",
    "fake = Faker()\n",
    "\n",
    "# Define categories and formulation types for the dataset\n",
    "categories = ['Foundation', 'Lipstick', 'Mascara', 'Eyeshadow', 'Blush']\n",
    "formulation_types = ['Liquid', 'Powder', 'Cream', 'Gel', 'Stick']\n",
    "primary_ingredients = ['Shea Butter', 'Hyaluronic Acid', 'Vitamin E', 'Collagen', 'Aloe Vera']\n",
    "status_options = ['Completed', 'In Progress', 'Failed', 'Pending']\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Product Formulations Dataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a list to store product data\n",
    "product_data = []\n",
    "product_ids = []  # To store unique ProductIDs for relationships\n",
    "\n",
    "# Generate 20 unique product formulations\n",
    "for _ in range(20):\n",
    "    product_id = fake.uuid4()  # Unique identifier for each product\n",
    "    product_ids.append(product_id)\n",
    "    product = {\n",
    "        \"ProductID\": product_id,\n",
    "        \"ProductName\": fake.word().capitalize() + \" \" + random.choice(categories),\n",
    "        \"Category\": random.choice(categories),\n",
    "        \"FormulationType\": random.choice(formulation_types),\n",
    "        \"PrimaryIngredients\": ', '.join(random.sample(primary_ingredients, 2)),\n",
    "        \"LaunchDate\": fake.date_between(start_date=\"-2y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    product_data.append(product)\n",
    "\n",
    "# Define schema for Product Formulations\n",
    "product_schema = StructType([\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"FormulationType\", StringType(), True),\n",
    "    StructField(\"PrimaryIngredients\", StringType(), True),\n",
    "    StructField(\"LaunchDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Product Formulations DataFrame\n",
    "product_formulations_df = spark.createDataFrame(product_data, product_schema)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Manufacturing Batch Dataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a list to store batch data\n",
    "batch_data = []\n",
    "\n",
    "# Generate 50 manufacturing batches\n",
    "for _ in range(50):\n",
    "    batch = {\n",
    "        \"BatchID\": fake.uuid4(),\n",
    "        \"ProductID\": random.choice(product_ids),  # Associate with a product\n",
    "        \"BatchDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\"),\n",
    "        \"Quantity\": random.randint(100, 1000),\n",
    "        \"Status\": random.choice(status_options)\n",
    "    }\n",
    "    batch_data.append(batch)\n",
    "\n",
    "# Define schema for Manufacturing Batches\n",
    "batch_schema = StructType([\n",
    "    StructField(\"BatchID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"BatchDate\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Manufacturing Batches DataFrame\n",
    "manufacturing_batch_df = spark.createDataFrame(batch_data, batch_schema)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Customer Feedback Dataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a list to store customer feedback data\n",
    "customer_feedback_data = []\n",
    "customer_ids = [fake.uuid4() for _ in range(30)]  # Generate 30 unique Customer IDs\n",
    "\n",
    "# Generate 50 customer feedback entries\n",
    "for _ in range(50):\n",
    "    feedback = {\n",
    "        \"FeedbackID\": fake.uuid4(),\n",
    "        \"ProductID\": random.choice(product_ids),\n",
    "        \"CustomerID\": random.choice(customer_ids),\n",
    "        \"Rating\": random.randint(1, 5),\n",
    "        \"Comments\": fake.sentence(),\n",
    "        \"FeedbackDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    customer_feedback_data.append(feedback)\n",
    "\n",
    "# Define schema for Customer Feedback\n",
    "customer_feedback_schema = StructType([\n",
    "    StructField(\"FeedbackID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"Rating\", IntegerType(), True),\n",
    "    StructField(\"Comments\", StringType(), True),\n",
    "    StructField(\"FeedbackDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Customer Feedback DataFrame\n",
    "customer_feedback_df = spark.createDataFrame(customer_feedback_data, customer_feedback_schema)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Sales Dataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a list to store sales data\n",
    "sales_data = []\n",
    "\n",
    "# Generate 50 sales transactions\n",
    "for _ in range(50):\n",
    "    sale = {\n",
    "        \"OrderID\": fake.uuid4(),\n",
    "        \"CustomerID\": random.choice(customer_ids),\n",
    "        \"ProductID\": random.choice(product_ids),\n",
    "        \"Quantity\": random.randint(1, 10),\n",
    "        \"TotalAmount\": round(random.uniform(20, 500), 2),\n",
    "        \"OrderDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    sales_data.append(sale)\n",
    "\n",
    "# Define schema for Sales Data\n",
    "sales_schema = StructType([\n",
    "    StructField(\"OrderID\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"TotalAmount\", FloatType(), True),\n",
    "    StructField(\"OrderDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Sales Data DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate Supplier Information Dataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a list to store supplier data\n",
    "supplier_data = []\n",
    "supplier_ids = [fake.uuid4() for _ in range(10)]  # Generate 10 unique Supplier IDs\n",
    "\n",
    "# Generate 20 supplier records\n",
    "for _ in range(20):\n",
    "    supplier = {\n",
    "        \"SupplierID\": random.choice(supplier_ids),\n",
    "        \"SupplierName\": fake.company(),\n",
    "        \"Material\": random.choice(primary_ingredients),\n",
    "        \"Cost\": round(random.uniform(10, 100), 2),\n",
    "        \"DeliveryDate\": fake.date_between(start_date=\"-1y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    supplier_data.append(supplier)\n",
    "\n",
    "# Define schema for Supplier Information\n",
    "supplier_schema = StructType([\n",
    "    StructField(\"SupplierID\", StringType(), True),\n",
    "    StructField(\"SupplierName\", StringType(), True),\n",
    "    StructField(\"Material\", StringType(), True),\n",
    "    StructField(\"Cost\", FloatType(), True),\n",
    "    StructField(\"DeliveryDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Supplier Information DataFrame\n",
    "supplier_df = spark.createDataFrame(supplier_data, supplier_schema)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Save DataFrames to CSV Files\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Define base path for saving files\n",
    "output_path = \"dbfs:/tmp/\"\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "product_formulations_df.coalesce(1).write.csv(output_path + \"product_formulations.csv\", header=True, mode=\"overwrite\")\n",
    "manufacturing_batch_df.coalesce(1).write.csv(output_path + \"manufacturing_batches.csv\", header=True, mode=\"overwrite\")\n",
    "customer_feedback_df.coalesce(1).write.csv(output_path + \"customer_feedback.csv\", header=True, mode=\"overwrite\")\n",
    "sales_df.coalesce(1).write.csv(output_path + \"sales_data.csv\", header=True, mode=\"overwrite\")\n",
    "supplier_df.coalesce(1).write.csv(output_path + \"supplier_information.csv\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41e487f-812b-40aa-b0c4-c4b6d9ac9d72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Upload csv datasets to landing zone"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded file to ADLS: data/incoming/customerfeedback/customerfeedback_20250128_225759.csv\n✅ Dataset 'customerfeedback' successfully uploaded to ADLS as data/incoming/customerfeedback/customerfeedback_20250128_225759.csv.\n✅ Uploaded file to ADLS: data/incoming/manufacturebatch/manufacturebatch_20250128_225759.csv\n✅ Dataset 'manufacturebatch' successfully uploaded to ADLS as data/incoming/manufacturebatch/manufacturebatch_20250128_225759.csv.\n✅ Uploaded file to ADLS: data/incoming/productformula/productformula_20250128_225759.csv\n✅ Dataset 'productformula' successfully uploaded to ADLS as data/incoming/productformula/productformula_20250128_225759.csv.\n✅ Uploaded file to ADLS: data/incoming/sales/sales_20250128_225759.csv\n✅ Dataset 'sales' successfully uploaded to ADLS as data/incoming/sales/sales_20250128_225759.csv.\n✅ Uploaded file to ADLS: data/incoming/supplier/supplier_20250128_225759.csv\n✅ Dataset 'supplier' successfully uploaded to ADLS as data/incoming/supplier/supplier_20250128_225759.csv.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Azure Storage connection details (using SAS Token for secure access)\n",
    "account_name = \"cdmo\"\n",
    "sas_token = \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-01-30T06:57:17Z&st=2025-01-28T22:57:17Z&spr=https&sig=o5zgDJJKAc%2BteYHegTuV8CSItQ2ICNrXofDiCpwaMTw%3D\"   # Replace with your SAS token\n",
    "container_name = \"00-landing\"\n",
    "\n",
    "# Construct the BlobServiceClient URL\n",
    "blob_service_url = f\"https://{account_name}.blob.core.windows.net?{sas_token}\"\n",
    "\n",
    "# Function to upload a file to Azure Data Lake Storage (ADLS)\n",
    "def upload_to_adls(local_path, blob_path):\n",
    "    \"\"\"\n",
    "    Uploads a file to Azure Data Lake Storage using SAS Token authentication.\n",
    "\n",
    "    Args:\n",
    "        local_path (str): Local file path to upload.\n",
    "        blob_path (str): Path in the ADLS container to upload the file to.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize BlobServiceClient using the SAS token URL\n",
    "        blob_service_client = BlobServiceClient(account_url=blob_service_url)\n",
    "\n",
    "        # Get the BlobClient for the target file in the specified container\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)\n",
    "\n",
    "        # Upload the file to ADLS\n",
    "        with open(local_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "        print(f\"✅ Uploaded file to ADLS: {blob_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error uploading file to ADLS: {e}\")\n",
    "\n",
    "# Define datasets and their corresponding ADLS folders\n",
    "datasets = {\n",
    "    \"customerfeedback\": customer_feedback_df,\n",
    "    \"manufacturebatch\": manufacturing_batch_df,\n",
    "    \"productformula\": product_formulations_df,\n",
    "    \"sales\": sales_df,\n",
    "    \"supplier\": supplier_df,\n",
    "}\n",
    "\n",
    "# Get the current date and time for file versioning\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "current_time = datetime.now().strftime(\"%H%M%S\")\n",
    "\n",
    "# Save DataFrames to DBFS and upload to ADLS\n",
    "for dataset_name, dataframe in datasets.items():\n",
    "    try:\n",
    "        # Define temporary paths for saving and accessing data\n",
    "        dbfs_temp_dir = f\"dbfs:/tmp/{dataset_name}\"  # Temporary DBFS directory for Spark output\n",
    "        local_temp_dir = f\"/dbfs/tmp/{dataset_name}\"  # Local path to access DBFS files\n",
    "        os.makedirs(local_temp_dir, exist_ok=True)\n",
    "\n",
    "        # Save the DataFrame to DBFS as a single CSV file\n",
    "        dataframe.coalesce(1).write.csv(dbfs_temp_dir, header=True, mode=\"overwrite\")\n",
    "\n",
    "        # Locate the part file in the DBFS directory\n",
    "        files = dbutils.fs.ls(dbfs_temp_dir)\n",
    "        part_file = next(f.path for f in files if f.name.startswith(\"part-\"))\n",
    "\n",
    "        # Rename the part file to a meaningful name with date and time\n",
    "        local_file_path = os.path.join(local_temp_dir, f\"{dataset_name}_{current_date}_{current_time}.csv\")\n",
    "        dbutils.fs.cp(part_file, f\"file:{local_file_path}\")\n",
    "\n",
    "        # Define the blob path in ADLS with the date folder and timestamped file name\n",
    "        adls_blob_path = f\"data/incoming/{dataset_name}/{dataset_name}_{current_date}_{current_time}.csv\"\n",
    "\n",
    "        # Upload the file to ADLS\n",
    "        upload_to_adls(local_file_path, adls_blob_path)\n",
    "\n",
    "        # Clean up the temporary DBFS directory\n",
    "        dbutils.fs.rm(dbfs_temp_dir, recurse=True)\n",
    "\n",
    "        print(f\"✅ Dataset '{dataset_name}' successfully uploaded to ADLS as {adls_blob_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing dataset '{dataset_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695ce2a3-946f-40cd-8f58-8b408fd635d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validate Datasets"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ProductID</th><th>ProductName</th><th>Category</th><th>FormulationType</th><th>PrimaryIngredients</th><th>LaunchDate</th></tr></thead><tbody><tr><td>c23ce5f9-d4bc-424c-80bb-c77aba34792c</td><td>Cause Lipstick</td><td>Eyeshadow</td><td>Stick</td><td>Hyaluronic Acid, Aloe Vera</td><td>2023-03-11</td></tr><tr><td>3a0b7164-faca-448b-8fdc-d029d84de1c0</td><td>Mean Foundation</td><td>Foundation</td><td>Liquid</td><td>Aloe Vera, Vitamin E</td><td>2024-09-21</td></tr><tr><td>284b1217-8440-4b2f-9a43-05e06681083f</td><td>Peace Foundation</td><td>Blush</td><td>Gel</td><td>Aloe Vera, Hyaluronic Acid</td><td>2023-06-02</td></tr><tr><td>e9adae79-e492-43ec-94d7-fd8ff80553b0</td><td>Focus Blush</td><td>Blush</td><td>Powder</td><td>Vitamin E, Aloe Vera</td><td>2024-01-11</td></tr><tr><td>0c724d89-48c2-4567-b359-3b541e56b488</td><td>Reveal Foundation</td><td>Foundation</td><td>Stick</td><td>Hyaluronic Acid, Shea Butter</td><td>2024-01-02</td></tr><tr><td>c63a6c06-e1e4-4c5b-8506-4ce98c723651</td><td>It Mascara</td><td>Lipstick</td><td>Cream</td><td>Hyaluronic Acid, Vitamin E</td><td>2024-02-21</td></tr><tr><td>6d868cb9-ce6c-4ae1-b4c2-75224ff5a6cc</td><td>Tree Eyeshadow</td><td>Eyeshadow</td><td>Liquid</td><td>Vitamin E, Collagen</td><td>2023-12-27</td></tr><tr><td>edb4d3d7-25b2-4b1c-9811-ff00f44d3b61</td><td>These Foundation</td><td>Blush</td><td>Liquid</td><td>Hyaluronic Acid, Vitamin E</td><td>2024-04-20</td></tr><tr><td>9153be43-d01b-43c4-a495-ddd94362edc2</td><td>She Foundation</td><td>Eyeshadow</td><td>Gel</td><td>Aloe Vera, Collagen</td><td>2023-07-24</td></tr><tr><td>8b685165-e4c6-4c50-963d-faebfdeaebb9</td><td>Message Foundation</td><td>Lipstick</td><td>Powder</td><td>Vitamin E, Collagen</td><td>2023-12-13</td></tr><tr><td>290c527a-6f90-40d1-831e-c71a77a6ad39</td><td>His Eyeshadow</td><td>Mascara</td><td>Stick</td><td>Aloe Vera, Shea Butter</td><td>2024-03-25</td></tr><tr><td>84f3de57-e5fa-41f2-9751-ce49e1190270</td><td>Your Foundation</td><td>Foundation</td><td>Powder</td><td>Shea Butter, Collagen</td><td>2023-09-28</td></tr><tr><td>6173ae8b-7756-44d8-8655-3010c023a2d2</td><td>Pick Foundation</td><td>Mascara</td><td>Gel</td><td>Hyaluronic Acid, Collagen</td><td>2023-03-18</td></tr><tr><td>13c3b2de-f807-45d3-8ee5-32526d6a2da1</td><td>Much Lipstick</td><td>Foundation</td><td>Powder</td><td>Aloe Vera, Collagen</td><td>2023-06-08</td></tr><tr><td>f3ea61dc-55e0-4a9f-b4b2-b51876be70bc</td><td>Tend Lipstick</td><td>Eyeshadow</td><td>Powder</td><td>Aloe Vera, Collagen</td><td>2023-09-04</td></tr><tr><td>50425103-c6b5-4161-a1a3-8a979efbeb31</td><td>Suggest Foundation</td><td>Foundation</td><td>Gel</td><td>Hyaluronic Acid, Shea Butter</td><td>2023-02-22</td></tr><tr><td>6c194a94-ffb7-41c8-97c4-94bfc19893e4</td><td>Education Foundation</td><td>Foundation</td><td>Gel</td><td>Vitamin E, Collagen</td><td>2024-11-25</td></tr><tr><td>8614443c-476a-4343-b70f-f84ab7bd05f5</td><td>Success Eyeshadow</td><td>Blush</td><td>Liquid</td><td>Aloe Vera, Shea Butter</td><td>2023-08-26</td></tr><tr><td>ec33ff6c-32c5-4823-9251-f74126c31389</td><td>Anything Mascara</td><td>Foundation</td><td>Liquid</td><td>Hyaluronic Acid, Vitamin E</td><td>2023-04-18</td></tr><tr><td>51516546-50c9-416b-a4ea-4ba929b99995</td><td>Various Blush</td><td>Eyeshadow</td><td>Liquid</td><td>Hyaluronic Acid, Vitamin E</td><td>2023-11-10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "c23ce5f9-d4bc-424c-80bb-c77aba34792c",
         "Cause Lipstick",
         "Eyeshadow",
         "Stick",
         "Hyaluronic Acid, Aloe Vera",
         "2023-03-11"
        ],
        [
         "3a0b7164-faca-448b-8fdc-d029d84de1c0",
         "Mean Foundation",
         "Foundation",
         "Liquid",
         "Aloe Vera, Vitamin E",
         "2024-09-21"
        ],
        [
         "284b1217-8440-4b2f-9a43-05e06681083f",
         "Peace Foundation",
         "Blush",
         "Gel",
         "Aloe Vera, Hyaluronic Acid",
         "2023-06-02"
        ],
        [
         "e9adae79-e492-43ec-94d7-fd8ff80553b0",
         "Focus Blush",
         "Blush",
         "Powder",
         "Vitamin E, Aloe Vera",
         "2024-01-11"
        ],
        [
         "0c724d89-48c2-4567-b359-3b541e56b488",
         "Reveal Foundation",
         "Foundation",
         "Stick",
         "Hyaluronic Acid, Shea Butter",
         "2024-01-02"
        ],
        [
         "c63a6c06-e1e4-4c5b-8506-4ce98c723651",
         "It Mascara",
         "Lipstick",
         "Cream",
         "Hyaluronic Acid, Vitamin E",
         "2024-02-21"
        ],
        [
         "6d868cb9-ce6c-4ae1-b4c2-75224ff5a6cc",
         "Tree Eyeshadow",
         "Eyeshadow",
         "Liquid",
         "Vitamin E, Collagen",
         "2023-12-27"
        ],
        [
         "edb4d3d7-25b2-4b1c-9811-ff00f44d3b61",
         "These Foundation",
         "Blush",
         "Liquid",
         "Hyaluronic Acid, Vitamin E",
         "2024-04-20"
        ],
        [
         "9153be43-d01b-43c4-a495-ddd94362edc2",
         "She Foundation",
         "Eyeshadow",
         "Gel",
         "Aloe Vera, Collagen",
         "2023-07-24"
        ],
        [
         "8b685165-e4c6-4c50-963d-faebfdeaebb9",
         "Message Foundation",
         "Lipstick",
         "Powder",
         "Vitamin E, Collagen",
         "2023-12-13"
        ],
        [
         "290c527a-6f90-40d1-831e-c71a77a6ad39",
         "His Eyeshadow",
         "Mascara",
         "Stick",
         "Aloe Vera, Shea Butter",
         "2024-03-25"
        ],
        [
         "84f3de57-e5fa-41f2-9751-ce49e1190270",
         "Your Foundation",
         "Foundation",
         "Powder",
         "Shea Butter, Collagen",
         "2023-09-28"
        ],
        [
         "6173ae8b-7756-44d8-8655-3010c023a2d2",
         "Pick Foundation",
         "Mascara",
         "Gel",
         "Hyaluronic Acid, Collagen",
         "2023-03-18"
        ],
        [
         "13c3b2de-f807-45d3-8ee5-32526d6a2da1",
         "Much Lipstick",
         "Foundation",
         "Powder",
         "Aloe Vera, Collagen",
         "2023-06-08"
        ],
        [
         "f3ea61dc-55e0-4a9f-b4b2-b51876be70bc",
         "Tend Lipstick",
         "Eyeshadow",
         "Powder",
         "Aloe Vera, Collagen",
         "2023-09-04"
        ],
        [
         "50425103-c6b5-4161-a1a3-8a979efbeb31",
         "Suggest Foundation",
         "Foundation",
         "Gel",
         "Hyaluronic Acid, Shea Butter",
         "2023-02-22"
        ],
        [
         "6c194a94-ffb7-41c8-97c4-94bfc19893e4",
         "Education Foundation",
         "Foundation",
         "Gel",
         "Vitamin E, Collagen",
         "2024-11-25"
        ],
        [
         "8614443c-476a-4343-b70f-f84ab7bd05f5",
         "Success Eyeshadow",
         "Blush",
         "Liquid",
         "Aloe Vera, Shea Butter",
         "2023-08-26"
        ],
        [
         "ec33ff6c-32c5-4823-9251-f74126c31389",
         "Anything Mascara",
         "Foundation",
         "Liquid",
         "Hyaluronic Acid, Vitamin E",
         "2023-04-18"
        ],
        [
         "51516546-50c9-416b-a4ea-4ba929b99995",
         "Various Blush",
         "Eyeshadow",
         "Liquid",
         "Hyaluronic Acid, Vitamin E",
         "2023-11-10"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ProductID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ProductName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "FormulationType",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PrimaryIngredients",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"yyyy-M-d\"}",
         "name": "LaunchDate",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to the specific file\n",
    "file_path = \"dbfs:/tmp/product_formulations.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-cdmo-datasets-creation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
