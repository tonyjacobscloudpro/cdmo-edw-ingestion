{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5069618-3bb7-414a-941c-6136a3443048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84cd264c-0d62-4413-a0cd-834b847129b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a date dimension in gold layer"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Date dimension successfully written to: wasbs://03-gold@cdmo.blob.core.windows.net/date_dimension\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, lit\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# 1. SPARK CONFIGURATION & AZURE STORAGE AUTHENTICATION\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Azure Storage account details (Using SAS Token)\n",
    "STORAGE_ACCOUNT_NAME = \"cdmo\"  # Replace with your storage account name\n",
    "SAS_TOKEN = \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2025-02-01T13:22:40Z&st=2025-01-31T05:22:40Z&spr=https&sig=eNjMZTrl03xT4e2cf5nA2fmHglRbbQaFYgTnqWaECF4%3D\"  # Replace with your SAS token\n",
    "CONTAINER_NAME = \"03-gold\"  # Target container for Gold layer\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DateDimensionLoad\").getOrCreate()\n",
    "\n",
    "# Configure Spark to use the SAS Token for authentication via `wasbs://`\n",
    "spark.conf.set(f\"fs.azure.sas.{CONTAINER_NAME}.{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\", SAS_TOKEN)\n",
    "\n",
    "# Define the Gold layer container path using `wasbs://`\n",
    "gold_layer_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.blob.core.windows.net/date_dimension\"\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# 2. GENERATE DATE DIMENSION DATAFRAME\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "def create_date_dimension(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Generates a date dimension DataFrame from start_date to end_date.\n",
    "    \"\"\"\n",
    "    date_df = spark.sql(f\"\"\"\n",
    "        SELECT explode(sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day)) as date\n",
    "    \"\"\")\n",
    "\n",
    "    date_dimension_df = (\n",
    "        date_df.withColumn(\"Year\", expr(\"year(date)\"))\n",
    "               .withColumn(\"Month\", expr(\"month(date)\"))\n",
    "               .withColumn(\"Day\", expr(\"day(date)\"))\n",
    "               .withColumn(\"DayOfWeek\", expr(\"date_format(date, 'EEEE')\"))\n",
    "               .withColumn(\"Quarter\", expr(\"quarter(date)\"))\n",
    "               .withColumn(\"WeekOfYear\", expr(\"weekofyear(date)\"))\n",
    "               .withColumn(\"IsWeekend\", expr(\"case when date_format(date, 'EEEE') IN ('Saturday', 'Sunday') then true else false end\"))\n",
    "               .withColumn(\"IsHoliday\", lit(False))  # Placeholder for specific holidays\n",
    "               .withColumn(\"MonthName\", expr(\"date_format(date, 'MMMM')\"))\n",
    "               .withColumn(\"DayOfMonth\", expr(\"dayofmonth(date)\"))\n",
    "               .withColumn(\"DayOfYear\", expr(\"dayofyear(date)\"))\n",
    "    )\n",
    "    return date_dimension_df\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# 3. CREATE AND WRITE DATE DIMENSION TO GOLD LAYER\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Define start and end dates\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2099-12-31\"\n",
    "\n",
    "# Create the date dimension DataFrame\n",
    "date_dimension_df = create_date_dimension(start_date, end_date)\n",
    "\n",
    "# Write the DataFrame to the Delta table in the Gold layer\n",
    "try:\n",
    "    date_dimension_df.write.format(\"delta\").mode(\"overwrite\").save(gold_layer_path)\n",
    "    print(f\"✅ Date dimension successfully written to: {gold_layer_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error writing to Gold layer: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "661a4b9e-5dde-4aec-9dc1-ca40f7655daa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validation"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded date dimension from: wasbs://03-gold@cdmo.blob.core.windows.net/date_dimension\n+----------+----+-----+---+---------+-------+----------+---------+---------+---------+----------+---------+\n|      date|Year|Month|Day|DayOfWeek|Quarter|WeekOfYear|IsWeekend|IsHoliday|MonthName|DayOfMonth|DayOfYear|\n+----------+----+-----+---+---------+-------+----------+---------+---------+---------+----------+---------+\n|2000-01-01|2000|    1|  1| Saturday|      1|        52|     true|    false|  January|         1|        1|\n|2000-01-02|2000|    1|  2|   Sunday|      1|        52|     true|    false|  January|         2|        2|\n|2000-01-03|2000|    1|  3|   Monday|      1|         1|    false|    false|  January|         3|        3|\n|2000-01-04|2000|    1|  4|  Tuesday|      1|         1|    false|    false|  January|         4|        4|\n|2000-01-05|2000|    1|  5|Wednesday|      1|         1|    false|    false|  January|         5|        5|\n|2000-01-06|2000|    1|  6| Thursday|      1|         1|    false|    false|  January|         6|        6|\n|2000-01-07|2000|    1|  7|   Friday|      1|         1|    false|    false|  January|         7|        7|\n|2000-01-08|2000|    1|  8| Saturday|      1|         1|     true|    false|  January|         8|        8|\n|2000-01-09|2000|    1|  9|   Sunday|      1|         1|     true|    false|  January|         9|        9|\n|2000-01-10|2000|    1| 10|   Monday|      1|         2|    false|    false|  January|        10|       10|\n+----------+----+-----+---+---------+-------+----------+---------+---------+---------+----------+---------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Define the correct path to the date dimension table in the Gold layer\n",
    "gold_layer_path = f\"wasbs://03-gold@{STORAGE_ACCOUNT_NAME}.blob.core.windows.net/date_dimension\"\n",
    "\n",
    "# Read the Delta table from the Gold layer\n",
    "try:\n",
    "    date_dimension_df = spark.read.format(\"delta\").load(gold_layer_path)\n",
    "    print(f\"✅ Successfully loaded date dimension from: {gold_layer_path}\")\n",
    "\n",
    "    # Display the first 10 rows of the DataFrame\n",
    "    date_dimension_df.show(10)  # Use .show(10) instead of .display()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading date dimension: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06-create-dim-date-gold",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
